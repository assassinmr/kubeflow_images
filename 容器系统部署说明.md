容器系统部署手册

[TOC]

## 1. 软硬件资源列表

### 1.1 软硬件兼容性

- 开发环境：
  - CPU 开启 VT-x/AMD-v
  - Win7/10 开启 Hyper-V支持
- 运行环境：
  - CPU 开启 VT-x/AMD-v
  - Win7/10 开启 Hyper-V支持
  - Linux 开启 KVM支持

### 1.2 虚拟机资源分配


| 服务器.域名 | IP地址            | 登陆                    | 说明       |
| ----------- | ----------------- | ----------------------- | ---------- |
|             | 220.192.168.57/24 | administrator/Padmin123 | 物理宿主机 |
| rdp-bdp1    | 172.30.80.21      | rdp/p@ssw0rd            | vm         |
| rdp-bdp3    | 172.30.80.23      | rdp/p@ssw0rd            | vm         |
| rdp-daa1    | 172.30.80.25      | rdp/p@ssw0rd            | vm         |
| rdp-daa2    | 172.30.80.26      | rdp/p@ssw0rd            | vm         |
| rdp-mgr1    | 172.30.80.31\|VIP | rdp/p@ssw0rd            | vm,master  |
| rdp-mgr2    | 172.30.80.32\|VIP | rdp/p@ssw0rd            | vm,master  |
| rdp-mgr3    | 172.30.80.33\|VIP | rdp/p@ssw0rd            | vm,master  |
| rdp-mgr4    | 172.30.80.34      | rdp/p@ssw0rd            | vm         |
| rdp-mgr5    | 172.30.80.35      | rdp/p@ssw0rd            | vm         |
| rdp-mgr6    | 172.30.80.36      | rdp/p@ssw0rd            | vm         |
| rdp-dev1    | 172.30.80.61      | rdp/p@ssw0rd            | vm         |
| rdp-dev2    | 172.30.80.62      | rdp/p@ssw0rd            | vm         |
| rdp-dev3    | 172.30.80.63      | rdp/p@ssw0rd            | vm         |
| rdp-dev4    | 172.30.80.64      | rdp/p@ssw0rd            | vm         |
| rdp-dev5    | 172.30.80.65      | rdp/p@ssw0rd            | vm         |
| rdp-demo    | 172.30.80.81      | rdp/p@ssw0rd            | vm         |
| gateway     | 172.30.80.88      | gateway/p@ssw0rd        | vm,gateway |

### 1.3 平台和软件选择 

- 容器：Docker
- 配置管理：
- 集群管理：Kubernetes
- 开发PaaS：

## 2. 容器集群部署

### 2.1 服务器虚拟机安装

服务器OS选择: ubuntu server 16.04.3

#### 2.1.1 OS初始化

- 每台VM虚拟机安装OS后，执行如下命令初始化hostname、软件源等：

```bash
sudo -i
# 安装并启用SSH
apt install ssh vim htop curl -y
# 根据“虚拟机资源分配”中的参数，设置静态IP
vi /etc/network/interfaces
# hostname最好设置为xxx.xxx的形式
vi /etc/hostname
#echo "rdp-mgr1.k8s" > /etc/hostname
# 不重启情况下使内核生效
#sysctl kernel.hostname=rdp-mgr1.k8s
# 绑定hosts，包括所有节点
vi /etc/hosts
#echo "127.0.0.1   rdp-mgr1.k8s" >> /etc/hosts
#echo "up route del default gw 192.168.52.2" >> /etc/network/interfaces
#echo "up route add default gw 172.30.80.88" >> /etc/network/interfaces
# 调整时区为 Asia/Chongqing
dpkg-reconfigure tzdata
#echo "Asia/Chongqing" > /etc/timezone
# 使用aliyun的镜像更新源
sed -i s/"deb cdrom"/"#deb cdrom"/g /etc/apt/sources.list
sed -i 's/us.archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list
apt update && sudo apt full-upgrade -yy

```

- 关闭Swap

```bash
swapoff -a 
sed -i 's/.*swap.*/#&/' /etc/fstab
```

- 禁用SeLinux和AppArmor



- 关闭防火墙`ufw disable`或者打开需要的网络接口

```bash
ufw enable
ufw default deny
# master nodes
ufw allow proto tcp to any port 22,80,443,6443,2379:2380,10250:10252,10255,30000:32767
# worker nodes
ufw allow proto tcp to any port 22,80,443,10250,10255,30000:32767
ufw reload

# Docker ports
ufw allow proto tcp to any port 2376,2377,7946,4789
ufw allow 7946/udp
```

Master node(s)

| Protocol | Direction | Port Range | Purpose                 |
| -------- | --------- | ---------- | ----------------------- |
| TCP      | Inbound   | 6443*      | Kubernetes API server   |
| TCP      | Inbound   | 2379-2380  | etcd server client API  |
| TCP      | Inbound   | 10250      | Kubelet API             |
| TCP      | Inbound   | 10251      | kube-scheduler          |
| TCP      | Inbound   | 10252      | kube-controller-manager |
| TCP      | Inbound   | 10255      | Read-only Kubelet API   |

Worker node(s)

| Protocol | Direction | Port Range  | Purpose               |
| -------- | --------- | ----------- | --------------------- |
| TCP      | Inbound   | 10250       | Kubelet API           |
| TCP      | Inbound   | 10255       | Read-only Kubelet API |
| TCP      | Inbound   | 30000-32767 | NodePort Services**   |

#### 2.1.2 配置节点之间互信

```bash
# 首先启用root登陆ssh，所有节点都需要配置
sudo passwd root
sudo passwd -u root 
sudo sed -i s/"PermitRootLogin prohibit-password"/"PermitRootLogin yes"/g /etc/ssh/sshd_config
sudo service ssh restart

# 在master节点1(172.30.80.31)上
sudo -i
ssh-keygen
ssh-copy-id 172.30.80.32
ssh-copy-id 172.30.80.33
#更多节点依此执行
```

#### 2.1.3 设置ntp时间同步

- 在172.30.80.88上配置 ntp server

```bash
apt install ntp && systemctl stop ntp

cat > /etc/ntp.conf <<EOF
# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help

driftfile /var/lib/ntp/ntp.drift

# Enable this if you want statistics to be logged.
#statsdir /var/log/ntpstats/

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

# Specify one or more NTP servers.

# Use servers from the NTP Pool Project. Approved by Ubuntu Technical Board
# on 2011-02-08 (LP: #104525). See http://www.pool.ntp.org/join.html for
# more information.
#pool 0.ubuntu.pool.ntp.org iburst
#pool 1.ubuntu.pool.ntp.org iburst
#pool 2.ubuntu.pool.ntp.org iburst
#pool 3.ubuntu.pool.ntp.org iburst

# Use Ubuntu's ntp server as a fallback.
#pool ntp.ubuntu.com

server 202.112.10.36 prefer          # 1.cn.pool.ntp.org
# 允许上层时间服务器主动修改本机时间
restrict 202.112.10.36 nomodify notrap noquery

server 127.127.1.0
fudge 127.127.1.0 stratum 10

# Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for
# details.  The web page <http://support.ntp.org/bin/view/Support/AccessRestrictions>
# might also be helpful.
#
# Note that "restrict" applies to both servers and clients, so a configuration
# that might be intended to block requests from certain clients could also end
# up blocking replies from your own upstream servers.

# By default, exchange time with everybody, but don't allow configuration.
restrict -4 default kod notrap nomodify nopeer noquery limited
restrict -6 default kod notrap nomodify nopeer noquery limited
restrict 172.30.0.0 mask 255.255.0.0 nomodify

# Local users may interrogate the ntp server more closely.
restrict 127.0.0.1
restrict ::1

# Needed for adding pool entries
restrict source notrap nomodify noquery

# Clients from this (example!) subnet have unlimited access, but only if
# cryptographically authenticated.
#restrict 192.168.123.0 mask 255.255.255.0 notrust


# If you want to provide time to your local subnet, change the next line.
# (Again, the address is an example only.)
#broadcast 192.168.123.255

# If you want to listen to time broadcasts on your local subnet, de-comment the
# next lines.  Please do this only if you trust everybody on the network!
#disable auth
#broadcastclient

#Changes recquired to use pps synchonisation as explained in documentation:
#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918

#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS
#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware

#server 127.127.22.1                   # ATOM(PPS)
#fudge 127.127.22.1 flag3 1            # enable PPS API

EOF

systemctl daemon-reload && systemctl start ntp
```

- 在需要时间同步的node上

```bash
apt install ntp && systemctl stop ntp
mv /etc/ntp.conf /etc/net.conf.bak
cat > /etc/ntp.conf <<EOF
# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help

driftfile /var/lib/ntp/ntp.drift

# Enable this if you want statistics to be logged.
#statsdir /var/log/ntpstats/

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

# Specify one or more NTP servers.
server 172.30.80.88
restrict 172.30.80.88 nomodify notrap noquery

# Use servers from the NTP Pool Project. Approved by Ubuntu Technical Board
# on 2011-02-08 (LP: #104525). See http://www.pool.ntp.org/join.html for
# more information.
#pool 0.ubuntu.pool.ntp.org iburst
#pool 1.ubuntu.pool.ntp.org iburst
#pool 2.ubuntu.pool.ntp.org iburst
#pool 3.ubuntu.pool.ntp.org iburst

# Use Ubuntu's ntp server as a fallback.
#pool ntp.ubuntu.com

server 127.127.1.0
fudge 127.127.1.0 stratum 10
# Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for
# details.  The web page <http://support.ntp.org/bin/view/Support/AccessRestrictions>
# might also be helpful.
#
# Note that "restrict" applies to both servers and clients, so a configuration
# that might be intended to block requests from certain clients could also end
# up blocking replies from your own upstream servers.

# By default, exchange time with everybody, but don't allow configuration.
restrict -4 default kod notrap nomodify nopeer noquery limited
restrict -6 default kod notrap nomodify nopeer noquery limited

# Local users may interrogate the ntp server more closely.
restrict 127.0.0.1
restrict ::1

# Needed for adding pool entries
restrict source notrap nomodify noquery

# Clients from this (example!) subnet have unlimited access, but only if
# cryptographically authenticated.
#restrict 192.168.123.0 mask 255.255.255.0 notrust


# If you want to provide time to your local subnet, change the next line.
# (Again, the address is an example only.)
#broadcast 192.168.123.255

# If you want to listen to time broadcasts on your local subnet, de-comment the
# next lines.  Please do this only if you trust everybody on the network!
#disable auth
#broadcastclient

#Changes recquired to use pps synchonisation as explained in documentation:
#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918

#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS
#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware

#server 127.127.22.1                   # ATOM(PPS)
#fudge 127.127.22.1 flag3 1            # enable PPS API

EOF

ntpdate 172.30.80.88
systemctl daemon-reload && systemctl start ntp
```

- 将时间写入系统时钟

```bash
hwclock -w
```



#### 2.1.4 搭建 http server

后期可以考虑建设配置中心/文件共享中心等来统一存放配置文件。

现阶段，在gateway上采用python3自带模块http.server建立最简单的http-server（每次重启gateway需要重新运行脚本）

```bash
#mkdir k8s-deploy && cd k8s-deploy
nohup python3 -m http.server &
# 默认服务器端口 8000
```

将所有需要的文件 sftp 上传到 k8s-deploy 目录

#### 2.1.5 配置LoadBalancer

- gateway1/gateway2上配置LVS-DR

```bash
apt install keepalived ipvsadm -y
```

```bash
cat >/etc/keepalived/keepalived.conf <<EOF
global_defs {
   router_id LVS_k8s_lb
}

vrrp_instance VI_1 {
    state ${HA_STATE}
    interface ${VIP_INTERFACE}
    virtual_router_id 51
    priority ${HA_PRIORITY}
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass rdplbha
    }
    virtual_ipaddress {
        ${KUBE_VIP}
    }
}

virtual_server fwmark 1 {
    delay_loop 6
    lb_algo wrr
    lb_kind DR
    persistence_timeout 0
    protocol TCP

    real_server 172.30.80.31 {
        weight 1
        TCP_CHECK {
            connect_timeout 10
            nb_get_retry 3
            delay_before_retry 3
            connect_port 6443
        }
    }

    real_server 172.30.80.32 {
        weight 1
        TCP_CHECK {
            connect_timeout 10
            nb_get_retry 3
            delay_before_retry 3
            connect_port 6443
        }
    }

    real_server 172.30.80.33 {
        weight 1
        TCP_CHECK {
            connect_timeout 10
            nb_get_retry 3
            delay_before_retry 3
            connect_port 6443
        }
    }
}

EOF

    modprobe ip_vs
    systemctl daemon-reload && systemctl restart keepalived.service
    ipvsadm -ln
```

一键安装脚本（主）

```bash
curl -L http://172.30.80.88:8000/shell/install-lb.sh | bash -s master vip=172.30.80.30
```

安装备服务器时将上述脚本中的第一个参数改为`replica`

- 节点上配置LVS-DR

```bash
modprobe ip_vs
    ln -fs /lib/systemd/system/rc-local.service /etc/systemd/system/rc-local.service

cat > /etc/systemd/system/rc-local.service <<EOF
#  This file is part of systemd.
#
#  systemd is free software; you can redistribute it and/or modify it
#  under the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation; either version 2.1 of the License, or
#  (at your option) any later version.

# This unit gets pulled automatically into multi-user.target by
# systemd-rc-local-generator if /etc/rc.local is executable.
[Unit]
Description=/etc/rc.local Compatibility
ConditionFileIsExecutable=/etc/rc.local
After=network.target

[Service]
Type=forking
ExecStart=/etc/rc.local start
TimeoutSec=0
RemainAfterExit=yes
GuessMainPID=no

[Install]
WantedBy=multi-user.target
Alias=rc-local.service

EOF

cat > /etc/rc.local <<EOF
#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will "exit 0" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.

ip addr add ${KUBE_VIP}/32 dev lo
route add -host ${KUBE_VIP} lo

exit 0

EOF

    chmod 755 /etc/rc.local
```

记得重启node

#### 2.1.6 部署NFS

```bash
apt install nfs-kernel-server -y
mkdir -p /nfsdata/k8s-nfs && chmod a+w /nfsdata/k8s-nfs
echo "/nfsdata/k8s-nfs *(rw,sync,crossmnt,no_subtree_check,no_root_squash) >> /etc/exports
exportfs -r
systemctl restart rpcbind && systemctl restart nfs-server.service
# showmount -e localhost
```

```bash
# Try it locally
apt install nfs-common 
mkdir /mnt/nfs
mount -t nfs -o proto=tcp,port=2049 172.30.80.90:/nfsdata /mnt/nfs
ls -l /mnt/nfs
```

#### 2.1.7 部署DNS

##### PowerDNS

- ~~安装 mariadb~~

```bash
apt install mariadb-server -y
mysql_secure_installation # mariadb 加固，设置 root 密码：p@ssw0rd
vi /etc/mysql/mariadb.conf.d/50-server.cnf
# 注释掉这一行
#bind-address           = 127.0.0.1
systemctl restart mysql.service
```

~~创建 powerdns数据库~~

```bash
mysql -u root -p #使用MariaDB root用户进行身份验证
 
CREATE DATABASE powerdns; #创建数据库
 
GRANT ALL ON powerdns.* TO 'powerdns_user'@'localhost' IDENTIFIED BY 'p@ssw0rd'; #创建一个名为“powerdns_user”的新用户并授予对数据库的访问权限
 
FLUSH PRIVILEGES; #刷新权限以更新用户设置
 
USE powerdns; #进入powerdns数据库
 
CREATE TABLE domains (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255) NOT NULL,
  master                VARCHAR(128) DEFAULT NULL,
  last_check            INT DEFAULT NULL,
  type                  VARCHAR(6) NOT NULL,
  notified_serial       INT UNSIGNED DEFAULT NULL,
  account               VARCHAR(40) CHARACTER SET 'utf8' DEFAULT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE UNIQUE INDEX name_index ON domains(name);


CREATE TABLE records (
  id                    BIGINT AUTO_INCREMENT,
  domain_id             INT DEFAULT NULL,
  name                  VARCHAR(255) DEFAULT NULL,
  type                  VARCHAR(10) DEFAULT NULL,
  content               VARCHAR(64000) DEFAULT NULL,
  ttl                   INT DEFAULT NULL,
  prio                  INT DEFAULT NULL,
  change_date           INT DEFAULT NULL,
  disabled              TINYINT(1) DEFAULT 0,
  ordername             VARCHAR(255) BINARY DEFAULT NULL,
  auth                  TINYINT(1) DEFAULT 1,
  PRIMARY KEY (id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE INDEX nametype_index ON records(name,type);
CREATE INDEX domain_id ON records(domain_id);
CREATE INDEX ordername ON records (ordername);


CREATE TABLE supermasters (
  ip                    VARCHAR(64) NOT NULL,
  nameserver            VARCHAR(255) NOT NULL,
  account               VARCHAR(40) CHARACTER SET 'utf8' NOT NULL,
  PRIMARY KEY (ip, nameserver)
) Engine=InnoDB CHARACTER SET 'latin1';


CREATE TABLE comments (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  name                  VARCHAR(255) NOT NULL,
  type                  VARCHAR(10) NOT NULL,
  modified_at           INT NOT NULL,
  account               VARCHAR(40) CHARACTER SET 'utf8' DEFAULT NULL,
  comment               TEXT CHARACTER SET 'utf8' NOT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE INDEX comments_name_type_idx ON comments (name, type);
CREATE INDEX comments_order_idx ON comments (domain_id, modified_at);


CREATE TABLE domainmetadata (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  kind                  VARCHAR(32),
  content               TEXT,
  PRIMARY KEY (id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE INDEX domainmetadata_idx ON domainmetadata (domain_id, kind);


CREATE TABLE cryptokeys (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  flags                 INT NOT NULL,
  active                BOOL,
  content               TEXT,
  PRIMARY KEY(id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE INDEX domainidindex ON cryptokeys(domain_id);


CREATE TABLE tsigkeys (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255),
  algorithm             VARCHAR(50),
  secret                VARCHAR(255),
  PRIMARY KEY (id)
) Engine=InnoDB CHARACTER SET 'latin1';

CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm);

quit; #退出
```

```bash
# if using InnoDB
ALTER TABLE records ADD CONSTRAINT `records_domain_id_ibfk` FOREIGN KEY (`domain_id`) REFERENCES `domains` (`id`) ON DELETE CASCADE ON UPDATE CASCADE;
ALTER TABLE comments ADD CONSTRAINT `comments_domain_id_ibfk` FOREIGN KEY (`domain_id`) REFERENCES `domains` (`id`) ON DELETE CASCADE ON UPDATE CASCADE;
ALTER TABLE domainmetadata ADD CONSTRAINT `domainmetadata_domain_id_ibfk` FOREIGN KEY (`domain_id`) REFERENCES `domains` (`id`) ON DELETE CASCADE ON UPDATE CASCADE;
ALTER TABLE cryptokeys ADD CONSTRAINT `cryptokeys_domain_id_ibfk` FOREIGN KEY (`domain_id`) REFERENCES `domains` (`id`) ON DELETE CASCADE ON UPDATE CASCADE;
```

- (postgre) 创建数据库

```bash
CREATE USER powerdns WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE powerdns OWNER powerdns;
GRANT ALL PRIVILEGES ON DATABASE powerdns TO powerdns;
```

- (postgre) Default Schema

```bash
CREATE TABLE domains (
  id                    SERIAL PRIMARY KEY,
  name                  VARCHAR(255) NOT NULL,
  master                VARCHAR(128) DEFAULT NULL,
  last_check            INT DEFAULT NULL,
  type                  VARCHAR(6) NOT NULL,
  notified_serial       INT DEFAULT NULL,
  account               VARCHAR(40) DEFAULT NULL,
  CONSTRAINT c_lowercase_name CHECK (((name)::TEXT = LOWER((name)::TEXT)))
);

CREATE UNIQUE INDEX name_index ON domains(name);


CREATE TABLE records (
  id                    BIGSERIAL PRIMARY KEY,
  domain_id             INT DEFAULT NULL,
  name                  VARCHAR(255) DEFAULT NULL,
  type                  VARCHAR(10) DEFAULT NULL,
  content               VARCHAR(65535) DEFAULT NULL,
  ttl                   INT DEFAULT NULL,
  prio                  INT DEFAULT NULL,
  change_date           INT DEFAULT NULL,
  disabled              BOOL DEFAULT 'f',
  ordername             VARCHAR(255),
  auth                  BOOL DEFAULT 't',
  CONSTRAINT domain_exists
  FOREIGN KEY(domain_id) REFERENCES domains(id)
  ON DELETE CASCADE,
  CONSTRAINT c_lowercase_name CHECK (((name)::TEXT = LOWER((name)::TEXT)))
);

CREATE INDEX rec_name_index ON records(name);
CREATE INDEX nametype_index ON records(name,type);
CREATE INDEX domain_id ON records(domain_id);
CREATE INDEX recordorder ON records (domain_id, ordername text_pattern_ops);


CREATE TABLE supermasters (
  ip                    INET NOT NULL,
  nameserver            VARCHAR(255) NOT NULL,
  account               VARCHAR(40) NOT NULL,
  PRIMARY KEY(ip, nameserver)
);


CREATE TABLE comments (
  id                    SERIAL PRIMARY KEY,
  domain_id             INT NOT NULL,
  name                  VARCHAR(255) NOT NULL,
  type                  VARCHAR(10) NOT NULL,
  modified_at           INT NOT NULL,
  account               VARCHAR(40) DEFAULT NULL,
  comment               VARCHAR(65535) NOT NULL,
  CONSTRAINT domain_exists
  FOREIGN KEY(domain_id) REFERENCES domains(id)
  ON DELETE CASCADE,
  CONSTRAINT c_lowercase_name CHECK (((name)::TEXT = LOWER((name)::TEXT)))
);

CREATE INDEX comments_domain_id_idx ON comments (domain_id);
CREATE INDEX comments_name_type_idx ON comments (name, type);
CREATE INDEX comments_order_idx ON comments (domain_id, modified_at);


CREATE TABLE domainmetadata (
  id                    SERIAL PRIMARY KEY,
  domain_id             INT REFERENCES domains(id) ON DELETE CASCADE,
  kind                  VARCHAR(32),
  content               TEXT
);

CREATE INDEX domainidmetaindex ON domainmetadata(domain_id);


CREATE TABLE cryptokeys (
  id                    SERIAL PRIMARY KEY,
  domain_id             INT REFERENCES domains(id) ON DELETE CASCADE,
  flags                 INT NOT NULL,
  active                BOOL,
  content               TEXT
);

CREATE INDEX domainidindex ON cryptokeys(domain_id);


CREATE TABLE tsigkeys (
  id                    SERIAL PRIMARY KEY,
  name                  VARCHAR(255),
  algorithm             VARCHAR(50),
  secret                VARCHAR(255),
  CONSTRAINT c_lowercase_name CHECK (((name)::TEXT = LOWER((name)::TEXT)))
);

CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm);
```

- 添加官方pdns repo，并安装

```bash
echo "deb [arch=amd64] http://repo.powerdns.com/ubuntu xenial-auth-41 main" >> /etc/apt/sources.list.d/pdns.list
echo "deb [arch=amd64] http://repo.powerdns.com/ubuntu xenial-rec-41 main" >> /etc/apt/sources.list.d/pdns.list
echo "deb [arch=amd64] http://repo.powerdns.com/ubuntu xenial-dnsdist-13 main" >> /etc/apt/sources.list.d/pdns.list

cat > /etc/apt/preferences.d/pdns <<EOF
Package: pdns-*
Pin: origin repo.powerdns.com
Pin-Priority: 600
EOF

cat > /etc/apt/preferences.d/dnsdist <<EOF
Package: dnsdist*
Pin: origin repo.powerdns.com
Pin-Priority: 600
EOF

curl https://repo.powerdns.com/FD380FBB-pub.asc | apt-key add - &&
apt update &&
apt install pdns-server pdns-backend-pgsql pdns-recursor dnsdist -y
```

- 编辑配置文件

/etc/powerdns/pdns.conf

```bash
launch+=gpgsql
gpgsql-host=172.30.80.103
gpgsql-port=5432
gpgsql-dbname=powerdns
gpgsql-user=powerdns
gpgsql-password=p@ssw0rd
gpgsql-dnssec=yes
local-address=172.30.80.89                 <- 监听地址
local-port=5300                            <- dns 请求端口
master=yes
webserver=yes                              <- 提供 API 接口
webserver-address=0.0.0.0
webserver-allow-from=0.0.0.0/0
webserver-port=8081                        <- api 端口
api=yes
api-key=pdnsapi
setgid=pdns
setuid=pdns
dnsupdate=yes
allow-dnsupdate-from=127.0.0.0/8,172.30.0.0/16
```

/etc/powerdns/recursor.conf

```bash
allow-from=0.0.0.0/0                             <- 允许所有客户端进行 DNS 请求
daemon=yes
forward-zones-file=/etc/powerdns/zone            <- 指定需要进行转发的域名 (由 zone 文件指定转发位置)
forward-zones-recurse=.=172.30.0.2               <- 没有指定的域名, 默认转发至 172.30.0.2
local-address=172.30.80.89                       <- 监听地址
local-port=53
server-id=ns.ssii.com
setgid=pdns
setuid=pdns
api-key=pdnsapi
```

/etc/powerdns/zone  

```bash
ssii.com=172.30.80.89:5300
```

- 从 BIND 导入

```bash
# 1.pdns 设置为 slave
# 2.在数据库中插入数据
INSERT INTO domains (name,type,master) VALUES ('ssii.com', 'SLAVE', '172.30.80.89');
# 3.启动 pdns
# 4.等待域名数据同步到 pdns
# 5.在数据库中更新数据
UPDATE domains set type='MASTER' where type='SLAVE';
# 6.pdns 设置为 master，并重启
```

- 安装 PowerDNS-Admin

编译安装过程参考：https://github.com/ngoduykhanh/PowerDNS-Admin/wiki/Running-PowerDNS-Admin-on-Ubuntu-16.04---Ubuntu-18.04

```bash
CREATE USER pdnsadmin WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE pdnsadmin OWNER pdnsadmin;
GRANT ALL PRIVILEGES ON DATABASE pdnsadmin TO pdnsadmin;
```

安装库文件

```bash
apt install postgresql-contrib python3-dev libsasl2-dev libldap2-dev libssl-dev libxml2-dev libxslt1-dev libxmlsec1-dev libffi-dev pkg-config
pip install psycopg2
```

Install yarn to build asset files

```bash
$ curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
$ echo "deb https://dl.yarnpkg.com/debian/ stable main" > /etc/apt/sources.list.d/yarn.list
$ apt-get update -y
$ apt-get install -y yarn
```

克隆源文件并安装

```bash
$ git clone https://github.com/ngoduykhanh/PowerDNS-Admin.git /opt/powerdns-admin
$ cd /opt/powerdns-admin
$ virtualenv -p python3 flask
$ . ./flask/bin/activate
(flask) [powerdns-admin] pip install -r requirements.txt
$ cp config_template.py config.py
```

In your `config.py` file, make sure you have

```bash
SQLALCHEMY_DATABASE_URI = 'postgresql://pdnsadmin:pdnsadmin@172.30.80.90/pdnsadmin'
```

Running

```bash
(flask) [powerdns-admin] export FLASK_APP=app/__init__.py
(flask) [powerdns-admin] flask db upgrade
(flask) [powerdns-admin] yarn install --pure-lockfile
(flask) [powerdns-admin] flask assets build
(flask) [powerdns-admin] ./run.py
```

Configure systemd service

```bash
$ sudo vim /etc/systemd/system/powerdns-admin.service

[Unit]
Description=PowerDNS-Admin
After=network.target

[Service]
User=root
Group=root
WorkingDirectory=/opt/powerdns-admin
ExecStart=/opt/powerdns-admin/flask/bin/gunicorn --workers 2 --bind unix:/opt/powerdns-admin/powerdns-admin.sock app:app

[Install]
WantedBy=multi-user.target
```

```bash
systemctl daemon-reload && systemctl enable powerdns-admin && systemctl start powerdns-admin
```

nginx configuration

```nginx
server {
  listen *:80;
  listen *:443 ssl;
  server_name               pdnsadmin.ssii.com;
  ssl_certificate           /opt/powerdns-admin/certs/pdnsadmin.cert.pem;
  ssl_certificate_key       /opt/powerdns-admin/certs/pdnsadmin.key.pem;
  ssl_protocols             TLSv1 TLSv1.1 TLSv1.2;
  ssl_ciphers               HIGH:!aNULL:!MD5;

  index                     index.html index.htm index.php;
  root                      /opt/powerdns-admin;
  access_log                /var/log/nginx/pdnsadmin.ssii.com.access.log combined;
  error_log                 /var/log/nginx/pdnsadmin.ssii.com.error.log;

  client_max_body_size              10m;
  client_body_buffer_size           128k;
  proxy_redirect                    off;
  proxy_connect_timeout             90;
  proxy_send_timeout                90;
  proxy_read_timeout                90;
  proxy_buffers                     32 4k;
  proxy_buffer_size                 8k;
  proxy_set_header                  Host $host;
  proxy_set_header                  X-Real-IP $remote_addr;
  proxy_set_header                  X-Forwarded-For $proxy_add_x_forwarded_for;
  proxy_headers_hash_bucket_size    64;

  location ~ ^/static/  {
    include  /etc/nginx/mime.types;
    root /opt/powerdns-admin/app;

    location ~*  \.(jpg|jpeg|png|gif)$ {
      expires 365d;
    }

    location ~* ^.+.(css|js)$ {
      expires 7d;
    }
  }

  location / {
    proxy_pass            http://unix:/opt/powerdns-admin/powerdns-admin.sock;
    proxy_read_timeout    120;
    proxy_connect_timeout 120;
    proxy_redirect        off;
  }

}
```

创建管理员账号

```bash
pdnsadmin/p@ssw0rd
```



##### BIND

```bash
apt install bind9 -y
```

修改 /etc/bind/named.conf.options

```bash
forwarders {
    172.30.0.2;
};

recursion yes;

dnssec-enable yes;
dnssec-validation yes;
dnssec-lookaside auto;

auth-nxdomain no;    # conform to RFC1035
listen-on-v6 { any; };

allow-query { any; };
allow-update { localhost; };
```

配置 zone 文件：

```bash
cat <<EOF >> /etc/bind/named.conf.local
zone "ssii.com" {
  type master;
  file "db.ssii.com";
};

zone "80.30.172.in-addr.arpa" IN {
  type master;
  file "db.172.30.80";
};

EOF

cat <<EOF > /var/cache/bind/db.ssii.com
$TTL 604800     ; 1 week
@       IN      SOA     ns.ssii.com. root.ssii.com. (
                                6          ; serial
                                604800     ; refresh (1 week)
                                86400      ; retry (1 day)
                                2419200    ; expire (4 weeks)
                                604800     ; minimum (1 week)
                                )
@       IN      NS      ns1.ssii.com.
@       IN      NS      ns2.ssii.com.
ns1     IN      A       172.30.80.89
ns2     IN      A       172.30.80.88
@       IN      A       172.30.80.100
jx      IN      A       172.30.80.100
www     IN      A       172.30.80.100
$ORIGIN jx.ssii.com.
*       IN      A       172.30.80.100
EOF

cat <<EOF > /var/cache/bind/db.172.30.80
$TTL    604800
@       IN      SOA     ssii.com. root.ssii.com. (
                              2         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
@       IN      NS      ns1.ssii.com.
@       IN      NS      ns2.ssii.com.
ns1     IN      A       172.30.80.89
ns2     IN      A       172.30.80.88
89      IN      PTR     ns1.ssii.com.
88      IN      PTR     ns2.ssii.com.
100     IN      PTR     ssii.com.
100     IN      PTR     jx.ssii.com.
EOF
```

```bash
systemctl restart bind9.service
```

检查配置是否正常

```bash
dig -t axfr ssii.com @172.30.80.89
```

配置客户端dns

```bash
sed -i s/"dns-nameservers 172.30.0.2"/"dns-nameservers 172.30.80.89"/g /etc/network/interfaces
systemctl restart networking.service
```

##### 主从 DNS

设置 powerdns 为 master，BIND 为 slave



#### 2.1.8 部署CA

~~dns-01证书申请（需外网域名）~~

```bash
wget https://dl.eff.org/certbot-auto
chmod u+x certbot-auto

./certbot-auto certonly  -d "*.jx.ssii.com" -d "*.ssii.com" -d "ssii.com" --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory
```

创建CA证书

```bash
# 简单的方式
openssl genrsa -out ca.key.pem 4096
chmod 400 ca.key.pem
openssl req -config openssl_root.cnf \
      -key ca.key.pem \
      -new -x509 -nodes -reqexts v3_req \
      -days 7300 -sha256 -extensions v3_ca \
      -out ca.cert.pem
chmod 444 ca.cert.pem
#根证书转换为PKCS#12格式，Win平台: certmgr.msc -> 导入到“受信任的根证书颁发机构”
openssl pkcs12 -export -cacerts -inkey ca.key.pem -in ca.cert.pem -out ca.p12

#cp /etc/ssl/openssl.cnf . #修改并增加域名
#openssl genrsa -out gitea.key.pem 2048
#openssl req -new -nodes -key gitea.key.pem -out gitea.csr -subj "/CN=gitea.ssii.com" -config openssl_root.cnf
#openssl x509 -req -in gitea.csr -CA ca.cert.pem -CAkey ca.key.pem -CAcreateserial -out gitea.cert.pem -days 3650 -extensions v3_req -extfile openssl_root.cnf
```

```bash
# 标准的方式，以下过程中所有密码均为：p@ssw0rd（或为空）

# step 1
mkdir -p /root/localca 
cd /root/localca
mkdir certs crl newcerts private
chmod 700 private
touch index.txt
echo 1000 > serial
## copy openssl.cnf and edit
openssl rand -out private/.rand 1000
openssl genrsa -aes256 -out private/ca.key.pem 4096
chmod 400 private/ca.key.pem
openssl req -config openssl_root.cnf \
      -key private/ca.key.pem \
      -new -x509 -days 7300 -sha256 -extensions v3_ca \
      -out certs/ca.cert.pem
chmod 444 certs/ca.cert.pem
#根证书转换为PKCS#12格式
#openssl  pkcs12 -export -cacerts -inkey private/ca.key.pem -in certs/ca.cert.pem -out certs/ca.p12

# step 2
mkdir -p /root/localca/intermediate 
cd /root/localca/intermediate
mkdir certs crl csr newcerts private
chmod 700 private
touch index.txt
echo 1000 > serial
echo 1000 > crlnumber
## copy openssl.cnf and edit
openssl rand -out private/.rand 1000
cd /root/localca
openssl genrsa -aes256 \
      -out intermediate/private/intermediate.key.pem 4096
chmod 400 intermediate/private/intermediate.key.pem
openssl req -config intermediate/openssl_inter.cnf -new -sha256 \
      -key intermediate/private/intermediate.key.pem \
      -out intermediate/csr/intermediate.csr.pem ##注意：Common Name与root ca不同
openssl ca -config intermediate/openssl_inter.cnf -extensions v3_intermediate_ca \
      -days 3650 -notext -md sha256 \
      -in intermediate/csr/intermediate.csr.pem \
      -out intermediate/certs/intermediate.cert.pem
chmod 444 intermediate/certs/intermediate.cert.pem

# step 3
cat intermediate/certs/intermediate.cert.pem \
      certs/ca.cert.pem > intermediate/certs/ca-chain.cert.pem
chmod 444 intermediate/certs/ca-chain.cert.pem

# 验证证书
openssl x509 -noout -text \
      -in intermediate/certs/intermediate.cert.pem
openssl verify -CAfile certs/ca.cert.pem \
      intermediate/certs/intermediate.cert.pem
```

（可选）手动申请服务器/客户端证书

```bash
cd /root/localca
openssl genrsa -aes256 \
      -out intermediate/private/jx.ssii.com.key.pem 2048
chmod 400 intermediate/private/jx.ssii.com.key.pem
openssl req -config intermediate/openssl_inter.cnf \
      -key intermediate/private/jx.ssii.com.key.pem \
      -new -sha256 -out intermediate/csr/jx.ssii.com.csr.pem
openssl ca -config intermediate/openssl_inter.cnf \
      -extensions server_cert -days 375 -notext -md sha256 \
      -in intermediate/csr/jx.ssii.com.csr.pem \
      -out intermediate/certs/jx.ssii.com.cert.pem
chmod 444 intermediate/certs/jx.ssii.com.cert.pem
# 验证证书
openssl x509 -noout -text \
      -in intermediate/certs/jx.ssii.com.cert.pem
openssl verify -CAfile intermediate/certs/ca-chain.cert.pem \
      intermediate/certs/jx.ssii.com.cert.pem
```

服务器/客户端证书可在需要的nginx/apache等配置文件中进行配置。

导入JRE证书库（根证书、服务器/客户端证书）

```bash
keytool -import -alias root.ssii.com -keystore %JRE_HOME/lib/security/cacerts -file ca.cert.pem
keytool -import -alias jx.ssii.com -keystore %JRE_HOME/lib/security/cacerts -file jx.crt
```

运行该命令时会提示输入密码，如果你没有改过的话密码是’changeit’
然后再运行下面的命令，就可以看到证书已经被加进去了。

```bash
keytool -list -keystore %JRE_HOME/lib/security/cacerts | less
```

将自建证书导入ubuntu服务器

```bash
cp ca.cert.pem /usr/local/share/ca-certificates/ca.crt
update-ca-certificates
```

```bash
cp /usr/local/share/ca-certificates/ca.crt /usr/share/ca-certificates/
dpkg-reconfigure ca-certificates
```

对于非ubuntu服务器

```bash
echo -n | openssl s_client -showcerts -connect jx.ssii.com:443 2>/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' >> /etc/ssl/certs/ca-certificates.crt
```

导入证书后需重启服务器/虚拟机/容器。

至此，k8s集群环境下，客户端导入ca证书链ca-chain.cert.pem即可。

1.) Add certificate to config map: lets say your pem file is my-cert.pem 

```bash
kubectl -n <namespace-for-config-map-optional> create configmap ca-pemstore --from-file=ca.cert.pem
```

2.) Mount configmap as volume to exiting CA root location of container: mount that config map’s file as one to one file relationship in volume mount in directory /etc/ssl/certs/ as file for example

```yaml
apiVersion: v1 
kind: Pod
metadata:
  name: cacheconnectsample
spec:
      containers:
      - name: cacheconnectsample
        image: cacheconnectsample:v1
        volumeMounts:
        - name: ca-pemstore
          mountPath: /etc/ssl/certs/my-cert.pem
          subPath: my-cert.pem
          readOnly: false
        ports:
        - containerPort: 80
        command: [ "dotnet" ]
        args: [ "cacheconnectsample.dll" ]
      volumes:
      - name: ca-pemstore
        configMap:
          name: ca-pemstore
```

#### 2.1.9 OpenLDAP

管理员登陆：cn=admin,dc=rdp,dc=dev 密码：p@ssw0rd

安装

```
apt install slapd ldap-utils
```

- *slapd* - 这个是 LDAP 的服务端程序；
- *ldap-utils* - 这个是一系列的操作工具，可选。 

> 安装完毕之后要求输入管理员密码，设置完毕之后默认情况下 `baseDN` 与登录名均为 `cn=admin,dc=nodomain`，当然我们可以利用下面方式重新配置。   

重新配置

利用以下命令可以重新配置，然后重新按照指示窗口设置一系列的内容，如: `baseDN` , `organization` , 管理员密码，等。

```
dpkg-reconfigure slapd
```

> 注意：弹出框的时候注意应该选择 `NO` 之后进行下一步，`YES` 表示不重新配置；另外，重新配置之后的之前的数据将会丢失，请注意备份。

查看命令

查看主数据库的 `baseDN` 相关信息，包括管理员账户，密码的 Hash 值等，其实这个命令查询出来的结果内容也可以从 `/etc/ldap/slapd/` 目录某个 `ldif` 文件中找到相匹配内容，但是不要直接编辑那些文件，而是利用上述重新配置的手段处理。

```
ldapsearch -Y EXTERNAL -H ldapi:/// -b olcDatabase={1}mdb,cn=config
```

重启服务使配置生效

```
systemctl restart slapd
```

客户端

- *Apache Directory Studio* - 重点推荐;
- *Jxplorer* - 不太推荐，功能比较单一；

配置文件

- `/usr/share/slapd/slapd.conf`

根RD：cn=admin,dc=rdp,dc=dev

### 2.2 单集群部署

#### 2.2.1 k8s版本选择

| Image Name                               | Release Version |
| ---------------------------------------- | --------------- |
| k8s.gcr.io/kube-apiserver-amd64          | v1.11.3         |
| k8s.gcr.io/kube-controller-manager-amd64 | v1.11.3         |
| k8s.gcr.io/kube-scheduler-amd64          | v1.11.3         |
| k8s.gcr.io/kube-proxy-amd64              | v1.11.3         |
| k8s.gcr.io/etcd-amd64                    | 3.2.18          |
| k8s.gcr.io/pause-amd64                   | 3.1             |
| k8s.gcr.io/k8s-dns-sidecar-amd64         | 1.14.10         |
| k8s.gcr.io/k8s-dns-kube-dns-amd64        | 1.14.10         |
| k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64   | 1.14.10         |
| quay.io/coreos/flannel                   | v0.10.0-amd64   |
| k8s.gcr.io/kubernetes-dashboard-amd64    | v1.10.0         |
| k8s.gcr.io/cluster-autoscaler            | v1.2.2          |
| quay.io/coreos/coredns                   | v1.1.2          |

#### 2.2.2 安装Docker

``` bash
sudo -i
# 安装官方最新版本
curl -fsSL "https://get.docker.com/" | sh

# 或者通过aliyun镜像安装稳定版本
# step 1: 安装必要的一些系统工具
apt-get update
apt-get -y install apt-transport-https ca-certificates curl software-properties-common
# step 2: 安装GPG证书
curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | apt-key add -
# Step 3: 写入软件源信息
add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
# Step 4: 更新并安装 Docker-CE
apt-get -y update
apt-get -y install docker-ce

# 或者安装指定版本的Docker-CE（推荐）:
# step 1 ~ step 3：同上
# step 4：安装 Docker-CE 17.03
apt-get update && sudo apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk '{print $3}')

# 设置系统升级时不更新docker-ce
apt-mark hold docker-ce

# 配置非root账户使用Docker
sudo usermod -aG docker $USER

# 修改为国内的镜像源，并使用加速器
curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://e24ee5b3.m.daocloud.io
systemctl restart docker.service
```



#### 2.2.3 预先下载工具和镜像

- ~~下载deb安装包~~

```bash
# 使用ustc的镜像源安装（暂时缺少gpg，安装不了）
#add-apt-repository "deb https://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-$(lsb_release -cs) main"

# 所以，直接从镜像源下载deb
kubernetes-cni_0.6.0-00_amd64.deb
kubeadm_1.11.2-00_amd64.deb
kubectl_1.11.2-00_amd64.deb
kubelet_1.11.2-00_amd64.deb
```

- 下载k8s镜像

```bash
# 脚本上传到服务器，通过Docker Hub拉取k8s镜像：

#!/bin/bash

set -x
dockerimages=(kube-apiserver-amd64:v1.11.3
kube-controller-manager-amd64:v1.11.3
kube-scheduler-amd64:v1.11.3
kube-proxy-amd64:v1.11.3
etcd-amd64:3.2.18
pause-amd64:3.1
k8s-dns-sidecar-amd64:1.14.10
k8s-dns-kube-dns-amd64:1.14.10
k8s-dns-dnsmasq-nanny-amd64:1.14.10
kubenetes-dashboard-amd64:v1.8.2)

j=1
for i in ${dockerimages[@]}
do
    echo $i
    echo $j

    docker pull arborhuang/$i
    docker tag arborhuang/$i k8s.gcr.io/$i 
    docker save k8s.gcr.io/$i -o $i.tar
    docker rmi arborhuang/$i

    let j+=1
done

# dashboard: if pull error
# docker pull arborhuang/dashboard:v1.10.0
# docker tag arborhuang/dashboard:v1.10.0 k8s.gcr.io/kubenetes-dashboard-amd64:v1.10.0
# docker save k8s.gcr.io/kubenetes-dashboard-amd64:v1.10.0 -o kubenetes-dashboard-amd64:v1.10.0.tar
# docker rmi arborhuang/dashboard:v1.10.0

set +x
```

- 将deb、image上传到临时文件共享服务器

#### 2.2.4 安装Kubernetes工具

- ~~手动安装cni插件~~

```bash
CNI_VERSION="v0.6.0"
mkdir -p /opt/cni/bin
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-amd64-${CNI_VERSION}.tgz" | tar -C /opt/cni/bin -xzf
```

- ~~手动安装kubeadm,kubelet,kubectl~~

```bash
RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"

mkdir -p /opt/bin
cd /opt/bin
curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl}
chmod +x {kubeadm,kubelet,kubectl}
echo "PATH=/opt/bin:$PATH" >> .profile
ln -s /opt/bin/* /usr/local/bin/

curl -sSL "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/kubelet.service" | sed "s:/usr/bin:/opt/bin:g" > /etc/systemd/system/kubelet.service
mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/10-kubeadm.conf" | sed "s:/usr/bin:/opt/bin:g" > /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

# 需要配置Cgroup与Docker一致，ubuntu下为cgroupfs，centos下为systemd，如下更改：
sed -i s/"cgroup-driver=cgroupfs"/"cgroup-driver=systemd"/g /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```

- ~~源码编译安装kubeadm,kubelet,kubectl~~

```bash
# 基于墙的原因，最终选择release编译的方式安装
apt install git -y
git clone https://github.com/kubernetes/release.git
docker build --tag=debian-packager debian
docker run --volume="$(pwd)/debian:/src" debian-packager
# 编译完成后生成deb包到debian/bin
```

- 安装依赖及deb包

```bash
# 预先下载cni kubectl kubelet kubeadm，通过sftp上传到服务器
apt install socat ebtables ethtool -y
dpkg -i kube*.deb

## 通过deb安装的话，以下配置也不修改，否则init出错
# 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf中增加或修改为：
#Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
# 并在ExecStart中增加启动参数 KUBELET_CGROUP_ARGS
```

更新：使用 aliyun 的镜像源来安装：

```bash
apt update && apt install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF  
apt update && apt install socat ebtables ethtool cri-tools -y
apt install -y kubelet kubeadm kubectl kubernetes-cni
```

- 设置开机启动并启动kubelet

```bash
sudo -i
systemctl enable kubelet
systemctl daemon-reload && systemctl restart kubelet
```

- 导入images

```bash
#!/bin/bash

KUBE_REPO_PREFIX=k8s.gcr.io

mkdir -p /tmp/k8s

images=(
    kube-apiserver-amd64_v1.11.3
    kube-controller-manager-amd64_v1.11.3
    kube-scheduler-amd64_v1.11.3
    kube-proxy-amd64_v1.11.3
    etcd-amd64_3.2.18
    pause-amd64_3.1
    k8s-dns-sidecar-amd64_1.14.10
    k8s-dns-kube-dns-amd64_1.14.10
    k8s-dns-dnsmasq-nanny-amd64_1.14.10
    kubenetes-dashboard-amd64_v1.10.0
)

for i in "${!images[@]}"; do
    ret=$(docker images | awk 'NR!=1{print $1"_"$2}'| grep $KUBE_REPO_PREFIX/${images[$i]} | wc -l)
    if [ $ret -lt 1 ];then
        curl -L http://$HTTP_SERVER/images/${images[$i]}.tar -o /tmp/k8s/${images[$i]}.tar
        docker load -i /tmp/k8s/${images[$i]}.tar
    fi
done

rm /tmp/k8s* -rf
```

#### 2.2.5 配置Kubernetes集群

##### 初始化master

配置主机的172.30.80.31为apiserver监听的地址，设置pod的网段为10.244.0.0/16

```bash
sudo -i
kubeadm reset
# 完全清除时执行：
#find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v
#rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd
systemctl daemon-reload && systemctl start kubelet

kubeadm init --apiserver-advertise-address=172.30.80.31 --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.11.3

# 如果用CoreDNS代替kube-dns，需要再加参数：--feature-gates=CoreDNS=true
# 或者集群安装后，使用 kubeadm upgrade apply <version> --feature-gates=CoreDNS=true
# 使用https://github.com/coredns/deployment/tree/master/kubernetes下的脚本
$ ./deploy.sh -r 10.244.0.0/16 | kubectl apply -f -
$ kubectl delete --namespace=kube-system deployment kube-dns
```

记录下token，如果忘记了可以在master上通过`kubeadmin token list`得到。注意：token有效期只有24h，超过了需要重新生成token

```bash
kubeadm token create --print-join-command
```

##### 配置非root用户使用kubectl命令

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

root用户使用kubectl命令

```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
```

##### 安装pod网络，使用flannel

```bash
sysctl net.bridge.bridge-nf-call-ip6tables=1
sysctl net.bridge.bridge-nf-call-iptables=1

kubectl apply -f http://172.30.80.88:8000/config/kube-flannel.yml
```

##### 配置master可以调度并运行pod

默认情况下，为了保证master的安全，master是不会被调度到app的。你可以取消这个限制

```bash
kubectl taint nodes --all node-role.kubernetes.io/master-
```

##### 加入节点

登录需要加入的节点机器上，以root运行kubeadm init运行后提示的命令

```bash
kubeadm join 172.30.80.31:6443 --token eb42d5.4faf3dd43c99c872 --discovery-token-ca-cert-hash sha256:72f601ee65870ad3bfe71dddfbe9fc1cd4dd94680807685c3c9c88f7546d5ea9
```

在master上执行：`kubectl get nodes`可以查看加入的节点

在node节点执行该条命令，如果出现以下错误：`The connection to the server localhost:8080 was refused - did you specify the right host or port?`需要在node上配置

```bash
mkdir -p $HOME/.kube
scp rdp@172.30.80.31:/home/rdp/.kube/config $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

##### 删除节点

```bash
kubectl drain $(hostname) --delete-local-data --force --ignore-daemonsets
kubectl delete node $(hostname)
kubeadm reset
```

### 2.3 HA部署

#### 2.3.1 安装kubernetes工具

参见2.2.1-2.2.4节。

#### 2.3.2 安装部署 etcd cluster

一个集群至少需要3个副本。可以选择下面两种部署方式中的一种：

1. 将 etcd cluster部署在独立的计算节点 (虚拟机)上，或者
2. 复用master节点。

第一种方式能够获得更好的性能。简单起见，我们采用第二种方式。

基本步骤

- 创建 etcd CA 证书
- 创建 etcd client 证书
- 配置节点之间的SSH登陆
- 创建 server 和 peer 证书（所有节点上操作）
- 运行 etcd 服务（所有节点上操作）

#####创建 etcd CA 证书

1. 首先安装 `cfssl` 和`cfssljson`:

   ```bash
   curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
   curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
   chmod +x /usr/local/bin/cfssl*
   ```

2. SSH 登陆`rdp-mgr1` 并执行:

   ```bash
   mkdir -p /etc/kubernetes/pki/etcd
   cd /etc/kubernetes/pki/etcd

   ```

   ```bash
   cat >ca-config.json <<EOF
   {
       "signing": {
           "default": {
               "expiry": "43800h"
           },
           "profiles": {
               "server": {
                   "expiry": "43800h",
                   "usages": [
                       "signing",
                       "key encipherment",
                       "server auth",
                       "client auth"
                   ]
               },
               "client": {
                   "expiry": "43800h",
                   "usages": [
                       "signing",
                       "key encipherment",
                       "client auth"
                   ]
               },
               "peer": {
                   "expiry": "43800h",
                   "usages": [
                       "signing",
                       "key encipherment",
                       "server auth",
                       "client auth"
                   ]
               }
           }
       }
   }
   EOF

   ```

   ```bash
   cat >ca-csr.json <<EOF
   {
       "CN": "etcd",
       "key": {
           "algo": "rsa",
           "size": 2048
       },
       "names": [
       	{
               "C": "CN",
               "ST": "ChengDu",
               "L": "ChengDu",
               "O": "k8s",
               "OU": "System"
       	}
       ]
   }
   EOF

   ```

   

3. 生成 CA 证书:

   ```bash
   cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

   ##### 输出
   ca.pem
   ca-key.pem
   ```

##### 创建 etcd client 证书

1. While on `rdp-mgr1`, run the following:

   ```bash
   cat >client.json <<EOF
   {
       "CN": "client",
       "key": {
           "algo": "ecdsa",
           "size": 256
       },
       "names": [
       	{
               "C": "CN",
               "ST": "ChengDu",
               "L": "ChengDu",
               "O": "k8s",
               "OU": "System"
       	}
       ]
   }
   EOF

   ```

   ```bash
   cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client

   ##### 输出
   client.pem
   client-key.pem
   ```



##### 配置节点之间的SSH登陆

在节点之间复制证书，需要开启用于`scp`的SSH免登陆

1. SSH登陆master各节点，并运行

   ```bash
   export PEER_NAME=$(hostname)
   export PRIVATE_IP=$(ip addr show ens38 | grep -Po 'inet \K[\d.]+')
   # 上面的ens38更改为各节点上用于部署apiserver的端口
   ```

2. 创建 SSH keys 

   ```bash
   ssh-keygen -t rsa -b 4096 -C "<email>"
   # <email> 可以为空
   ```

3. 分别输出 `rdp-mgr2` 和`rdp-mgr3`上的key

   ```bash
    cat ~/.ssh/id_rsa.pub
   ```

4. 复制上面输出的内容到 `rdp-mgr1`的文件 `~/.ssh/authorized_keys` 中

##### 创建 server 和 peer 证书

1. 同步 ca 和 client 的证书相关文件到所有的master节点

   ```bash
   mkdir -p /etc/kubernetes/pki/etcd
   cd /etc/kubernetes/pki/etcd
   scp root@172.30.80.31:/etc/kubernetes/pki/etcd/ca.pem .
   scp root@172.30.80.31:/etc/kubernetes/pki/etcd/ca-key.pem .
   scp root@172.30.80.31:/etc/kubernetes/pki/etcd/client.pem .
   scp root@172.30.80.31:/etc/kubernetes/pki/etcd/client-key.pem .
   scp root@172.30.80.31:/etc/kubernetes/pki/etcd/ca-config.json .

   ```

2. 在所有的 master (etcd) 节点上操作

   ```bash
   cfssl print-defaults csr > config.json
   # sed -i '0,/CN/{s/example\.net/'"$PEER_NAME"'/}' config.json
   # sed -i 's/www\.example\.net/'"$LOCAL_IP"'/' config.json
   # sed -i 's/example\.net/'"$LOCAL_IP"'/' config.json

   cat > config.json <<EOF
   {
       "CN": "$PEER_NAME",
       "hosts": [
           "$LOCAL_IP"
       ],
       "key": {
           "algo": "ecdsa",
           "size": 256
       },
       "names": [
           {
               "C": "CN",
               "ST": "ChengDu",
               "L": "ChengDu",
               "O": "k8s",
               "OU": "System"
       	}
       ]
   }
   EOF

   cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | cfssljson -bare server
   cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | cfssljson -bare peer

   ```

   会将默认配置替换为master的 hostname 和 IP 地址，在创建证书之前，务必确保这些信息是正确的。如果创建时出现错误，需要重新配置 `config.json` 并重新运行 `cfssl` 命令。

   将会输出这些文件： `peer.pem`, `peer-key.pem`, `server.pem`, `server-key.pem`

##### 运行 etcd 服务（所有master节点上操作）

有两种方式运行 etcd

- systemd

第一种方式，安装为系统服务。

1. 首先下载并安装etcd可执行程序:

   ```bash
   export ETCD_VERSION=v3.2.18
   curl -sSL http://$HTTP_SERVER/etcd/etcd-${ETCD_VERSION}-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/local/bin/
   rm -rf etcd-$ETCD_VERSION-linux-amd64*

   ```

   

2. 然后创建 systemd 所需要的配置文件:

   ```bash
   touch /etc/etcd.env
   echo "" > /etc/etcd.env
   echo "PEER_NAME=$PEER_NAME" >> /etc/etcd.env
   echo "PRIVATE_IP=$PRIVATE_IP" >> /etc/etcd.env

   ```

3. 创建 systemd 启动文件:

   ```bash
   cat >/etc/systemd/system/etcd.service <<EOF
   [Unit]
   Description=etcd
   After=network.target
   After=network-online.target
   Wants=network-online.target
   Documentation=https://github.com/coreos/etcd
   Conflicts=etcd.service
   Conflicts=etcd2.service

   [Service]
   EnvironmentFile=/etc/etcd.env
   Type=notify
   Restart=always
   RestartSec=5s
   LimitNOFILE=40000
   TimeoutStartSec=0

   ExecStart=/usr/local/bin/etcd --name ${PEER_NAME} \
       --data-dir /var/lib/etcd \
       --listen-client-urls https://${PRIVATE_IP}:2379,https://127.0.0.1:2379 \
       --advertise-client-urls https://${PRIVATE_IP}:2379 \
       --listen-peer-urls https://${PRIVATE_IP}:2380 \
       --initial-advertise-peer-urls https://${PRIVATE_IP}:2380 \
       --cert-file=/etc/kubernetes/pki/etcd/server.pem \
       --key-file=/etc/kubernetes/pki/etcd/server-key.pem \
       --client-cert-auth \
       --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
       --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem \
       --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem \
       --peer-client-cert-auth \
       --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
       --initial-cluster <etcd0>=https://<etcd0-ip-address>:2380,<etcd1>=https://<etcd1-ip-address>:2380,<etcd2>=https://<etcd2-ip-address>:2380 \
       --initial-cluster-token my-etcd-token \
       --initial-cluster-state new

   [Install]
   WantedBy=multi-user.target
   EOF

   ```

   替换 `<etcd0-ip-address>`, `<etcd1-ip-address>` and `<etcd2-ip-address>` 为 etcd 集群主机的 IPv4 地址

4. 最后启动 etcd 服务:

   ```bash
   systemctl daemon-reload
   systemctl start etcd

   ```

5. 确认 etcd 服务已成功运行了:

   ```bash
   systemctl status etcd
   ```

- Static Pods

第二种方式，跟随master init 运行。

1. 生成 manifest 文件

   ```bash
   mkdir -p /etc/kubernetes/manifests

   cat <<EOF >/etc/kubernetes/manifests/etcd.yaml
   apiVersion: v1
   kind: Pod
   metadata:
   labels:
       component: etcd
       tier: control-plane
   name: ${ETCD_PODNAME}
   namespace: kube-system
   spec:
   containers:
   - command:
       - etcd --name ${PEER_NAME} \
       - --data-dir /var/lib/etcd \
       - --listen-client-urls https://${LOCAL_IP}:2379 \
       - --advertise-client-urls https://${LOCAL_IP}:2379 \
       - --listen-peer-urls https://${LOCAL_IP}:2380 \
       - --initial-advertise-peer-urls https://${LOCAL_IP}:2380 \
       - --cert-file=/certs/server.pem \
       - --key-file=/certs/server-key.pem \
       - --client-cert-auth \
       - --trusted-ca-file=/certs/ca.pem \
       - --peer-cert-file=/certs/peer.pem \
       - --peer-key-file=/certs/peer-key.pem \
       - --peer-client-cert-auth \
       - --peer-trusted-ca-file=/certs/ca.pem \
       - --initial-cluster <etcd0>=https://<etcd0-ip-address>:2380,<etcd1>=https://<etcd1-ip-address>:2380,<etcd2>=https://<etcd2-ip-address>:2380 \
       - --initial-cluster-token rdpetcd \
       - --initial-cluster-state new
       image: k8s.gcr.io/etcd-amd64:3.2.18
       livenessProbe:
       httpGet:
           path: /health
           port: 2379
           scheme: HTTP
       initialDelaySeconds: 15
       timeoutSeconds: 15
       name: etcd
       env:
       - name: PUBLIC_IP
       valueFrom:
           fieldRef:
           fieldPath: status.hostIP
       - name: PRIVATE_IP
       valueFrom:
           fieldRef:
           fieldPath: status.podIP
       - name: PEER_NAME
       valueFrom:
           fieldRef:
           fieldPath: metadata.name
       volumeMounts:
       - mountPath: /var/lib/etcd
       name: etcd
       - mountPath: /certs
       name: certs
   hostNetwork: true
   volumes:
   - hostPath:
       path: /var/lib/etcd
       type: DirectoryOrCreate
       name: etcd
   - hostPath:
       path: /etc/kubernetes/pki/etcd
       name: certs
   EOF

   ```

   替换如下的内容：

   - `<podname>` 替换为节点名称 (e.g. `rdp-mgr1.k8s`, `rdp-mgr3.k8s` or `rdp-mgr3.k8s`)
   - `<etcd0-ip-address>`, `<etcd1-ip-address>` and `<etcd2-ip-address>` 替换为需要运行 etcd 的节点IP。

##### kubeadm 部署 etcd cluster

The general approach is to generate all certs on one node and only distribute the *necessary* files to the other nodes.

> **Note:**  kubeadm contains all the necessary crytographic machinery to generate the certificates described below; no other cryptographic tooling is required for this example.

###### Configure the kubelet to be a service manager for etcd.

Running etcd is simpler than running kubernetes so you must override the kubeadm-provided kubelet unit file by creating a new one with a higher precedence.

```
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
```

###### Create configuration files for kubeadm.

Generate one kubeadm configuration file for each host that will have an etcd member running on it using the following script.

```
# Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts
export HOST0=172.30.80.31
export HOST1=172.30.80.32
export HOST2=172.30.80.33

# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1alpha2"
kind: MasterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF
done
```

###### Generate the certificate authority

If you already have a CA then the only action that is copying the CA’s `crt` and `key` file to `/etc/kubernetes/pki/etcd/ca.crt` and `/etc/kubernetes/pki/etcd/ca.key`. After those files have been copied, please skip this step.

If you do not already have a CA then run this command on `$HOST0` (where you generated the configuration files for kubeadm).

```
kubeadm alpha phase certs etcd-ca
```

This creates two files

- `/etc/kubernetes/pki/etcd/ca.crt`
- `/etc/kubernetes/pki/etcd/ca.key`

###### Create certificates for each member

```
kubeadm alpha phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete
```

###### Copy certificates and kubeadm configs

The certificates have been generated and now they must be moved to their respective hosts.

```
 USER=ubuntu
 HOST=${HOST1}
 scp -r /tmp/${HOST}/* ${USER}@${HOST}:
 ssh ${USER}@${HOST}
 USER@HOST $ sudo -Es
 root@HOST $ chown -R root:root pki
 root@HOST $ mv pki /etc/kubernetes/
```

###### Ensure all expected files exist

The complete list of required files on `$HOST0` is:

```
/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
```

On `$HOST1`:

```
$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
```

On `$HOST2`

```
$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
```

###### Create the static pod manifests

Now that the certificates and configs are in place it’s time to create the manifests. On each host run the `kubeadm` command to generate a static manifest for etcd.

```
root@HOST0 $ kubeadm alpha phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@HOST1 $ kubeadm alpha phase etcd local --config=/home/ubuntu/kubeadmcfg.yaml
root@HOST2 $ kubeadm alpha phase etcd local --config=/home/ubuntu/kubeadmcfg.yaml
```

###### Optional: Check the cluster health

```
docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://${HOST0}:2379 cluster-health
...
cluster is healthy
```

#### 2.3.3 安装并配置keepalived

##### 安装keepalived

```bash
apt install -y keepalived

```

##### 配置keepalived.conf

```bash
cat >/etc/keepalived/keepalived.conf <<EOF
global_defs {
	router_id LVS_k8s
}

vrrp_script CheckK8sMaster {
    script "curl -k https://172.30.80.31:6443"
    interval 3
    timeout 9
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state <ha_state>
    interface ens38
    virtual_router_id 61
    # 主节点权重最高 依次减少
    priority 120
    advert_int 1
    #修改为本地IP 
    mcast_src_ip 172.30.80.31
    nopreempt
    authentication {
        auth_type PASS
        auth_pass rdpha
    }
    unicast_peer {
        #注释掉本地IP
        #172.30.80.31
        172.30.80.32
        172.30.80.33
    }
    virtual_ipaddress {
    	172.30.80.30/16
    }
    track_script {
    	CheckK8sMaster
    }
}

EOF
```

- `<ha-state>` 替换为 `MASTER`

##### 启动keepalived

```bash
modprobe ip_vs
systemctl daemon-reload && systemctl restart keepalived.service
```

##### 依次配置其它master节点

修改对应节点IP，`<ha-state>` 替换为 `BACKUP`

节点权重

- rdp-mgr1 priority 120
- rdp-mgr2 priority 110
- rdp-mgr3 priority 100

#### 2.3.4  初始化第一个master节点

1. 编写 kubeadm 初始化所需要的配置文件:

   ```bash
   cat >config.yaml <<EOF
   apiVersion: kubeadm.k8s.io/v1alpha1
   kind: MasterConfiguration
   api:
     advertiseAddress: <private-ip>
   etcd:
     endpoints:
     - https://<etcd0-ip-address>:2379
     - https://<etcd1-ip-address>:2379
     - https://<etcd2-ip-address>:2379
     caFile: /etc/kubernetes/pki/etcd/ca.pem
     certFile: /etc/kubernetes/pki/etcd/client.pem
     keyFile: /etc/kubernetes/pki/etcd/client-key.pem
   networking:
     podSubnet: <podCIDR>
   apiServerCertSANs:
   - <load-balancer-ip>
   kubernetesVersion: <kubernetes-version>
   apiServerExtraArgs:
     apiserver-count: 3
   
   EOF
   ```

   - `<private-ip>` 替换为 master 节点的 IPv4
   - `<etcd0-ip>`, `<etcd1-ip>` and `<etcd2-ip>` 替换为各 etcd 节点的 IP 地址
   - `<podCIDR>`  Pod CIDR.  CNI provider 使用 flannel, 所以设置为10.244.0.0/16
   - `<load-balancer-ip>`  替换为VIP，这里是172.30.80.30
   - `<kubernetes-version>` 替换为k8s版本，这里是 v1.11.3

   **Note:** 如果部署 Kubernetes 1.9+, 可以替换 `apiserver-count: 3` 为 `endpoint-reconciler-type: lease`

2. 运行 kubeadm init

   ```bash
   kubeadm init --config=config.yaml
   cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   ```

#### 2.3.5 初始化其它的 master 节点

##### 同步 CA 证书

在运行 kubeadm 初始化前，先同步 `master0` 上的 K8s CA 证书到其它 master

Option 1: Copy with scp

1. 首先确保 master 之间 ssh 免登陆

2. 同步 CA 证书，在各节点上执行：

   ```bash
   scp root@<master0-ip-address>:/etc/kubernetes/pki/* /etc/kubernetes/pki
   rm apiserver.crt

   ```

Option 2: Copy paste

1. 直接拷贝 `/etc/kubernetes/pki/ca.crt` 和`/etc/kubernetes/pki/ca.key` 中的内容，到其它 master 节点上手动创建相应文件

##### 节点初始化

参照 2.3.4 节内容。

#### 2.3.6 添加各 master 节点到 load balancer

初始化完成后，参照 2.3.3 节配置 keepalived，并验证keepalived是否正常运行。

#### 2.3.7 安装 CNI 网络插件

配置cni参数

```bash
cat >/etc/cni/net.d/10-mynet.conf <<EOF
{
    "cniVersion": "0.6.0",
    "name": "mynet",
    "type": "bridge",
    "bridge": "cni0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "10.244.0.0/16",
        "routes": [
            {"dst": "0.0.0.0/0"}
        ]
    }
}
EOF
cat >/etc/cni/net.d/99-loopback.conf <<EOF
{
    "cniVersion": "0.6.0",
    "type": "loopback"
}
EOF
```

参照 2.2.5 节中“3、安装pod网络，使用flannel”，安装 CNI 网络插件。

#### 2.3.8 初始化 workers

至少需要初始化3个 worker 节点

参照单节点 master 的配置方式 2.2.5 节中“5、加入节点”。

#### 2.3.9 更新配置

1. 重新配置 kube-proxy 通过 load balancer 访问 kube-apiserver

   master节点上执行：

   ```bash
   kubectl get configmap -n kube-system kube-proxy -o yaml > kube-proxy-cm.yaml
   sed -i 's#server:.*#server: https://<masterLoadBalancerFQDN>:6443#g' kube-proxy-cm.yaml
   kubectl apply -f kube-proxy-cm.yaml --force
   # restart all kube-proxy pods to ensure that they load the new configmap
   kubectl delete pod -n kube-system -l k8s-app=kube-proxy

   ```

2. 重新配置 kubelet 通过 load balancer 访问 kube-apiserver

   worker节点上执行：

   ```bash
   sudo sed -i 's#server:.*#server: https://<masterLoadBalancerFQDN>:6443#g' /etc/kubernetes/kubelet.conf
   sudo systemctl restart kubelet
   ```

### 2.4 通过脚本一键部署

#### 2.4.1 http.server

在一台单独的server上启动一个http-server，用来存放config、image和deb包等文件，脚本会从此处拉取文件。

后期可以考虑建设配置中心/文件共享中心等来统一存放配置文件。

现阶段，在gateway上采用python3自带模块http.server建立最简单的http-server（每次重启gateway需要重新运行脚本）

```bash
nohup python3 -m http.server &
```

http-server地址：http://172.30.80.88:8000/

```bash
~/k8s-deploy$ tree
.
├── bin
│   ├── cfssl
│   ├── cfssljson
│   └── kubeadm
├── certs
│   ├── ca.cert.pem
│   ├── ca.key.pem
│   ├── ca.p12
│   ├── ca.srl
│   ├── jx.crt
│   ├── jx.csr
│   ├── jx.key
│   ├── openssl_inter.cnf
│   └── openssl_root.cnf
├── config
│   ├── cert-manager
│   │   ├── cert-dashboard.yaml
│   │   ├── cert-ingress.yaml
│   │   ├── cert-jx-acme.yaml
│   │   ├── cert-jx.yaml
│   │   ├── cert-kubeapps.yaml
│   │   └── issuer.yaml
│   ├── chartmuseum
│   │   └── myvalues.yaml
│   ├── dashboard
│   │   ├── dashboard-admin.yaml
│   │   ├── dashboard-ingress.yaml
│   │   ├── kubernetes-dashboard.yaml
│   │   └── myvalues.yaml
│   ├── docker-registry
│   │   └── myvalues.yaml
│   ├── gitea
│   │   └── myvalues.yaml
│   ├── heapster
│   │   └── myvalues.yaml
│   ├── ingress
│   │   ├── baremetal-service.yaml
│   │   ├── ingress-nginx.yaml
│   │   └── myvalues.yaml
│   ├── jx-ingress
│   │   ├── chartmuseum.ig.yaml
│   │   ├── docker-registry.ig.yaml
│   │   ├── gitea-gitea.ig.yaml
│   │   ├── jenkins.ig.yaml
│   │   ├── monocular.ig.yaml
│   │   └── nexus.ig.yaml
│   ├── keycloak
│   │   └── myvalues.yaml
│   ├── kube
│   │   └── config
│   ├── kubeapps
│   │   └── ingress.yaml
│   ├── kube-flannel.yml
│   ├── ldap
│   │   └── ldap-example-users.ldif
│   ├── mariadb
│   │   └── myvalues.yaml
│   ├── metallb
│   │   ├── metallb-config.yaml
│   │   └── metallb.yaml
│   ├── monocular
│   │   └── myvalues.yaml
│   ├── mysql
│   │   └── myvalues.yaml
│   └── nfs-client
│       ├── auth
│       │   ├── clusterrolebinding.yaml
│       │   ├── clusterrole.yaml
│       │   └── serviceaccount.yaml
│       ├── class.yaml
│       └── deployment.yaml
├── coredns
│   ├── coredns.yaml.sed
│   ├── deploy.sh
│   └── README.md
├── debs
│   ├── cri-tools_1.11.1-00_amd64.deb
│   ├── debs.tar.gz
│   ├── kubeadm_1.11.3-00_amd64.deb
│   ├── kubectl_1.11.3-00_amd64.deb
│   ├── kubelet_1.11.3-00_amd64.deb
│   └── kubernetes-cni_0.6.0-00_amd64.deb
├── etcd
│   └── etcd-v3.2.24-linux-amd64.tar.gz
├── images
│   ├── addon-resizer_1.7.tar
│   ├── cluster-autoscaler_v1.3.1.tar
│   ├── coredns_1.1.3.tar
│   ├── defaultbackend_1.4.tar
│   ├── flannel_v0.10.0-amd64.tar
│   ├── heapster-amd64_v1.5.2.tar
│   ├── heapster-grafana-amd64_v4.4.3.tar
│   ├── heapster-influxdb-amd64_v1.3.3.tar
│   ├── heapster_v1.3.0.tar
│   ├── kube-addon-manager_v8.6.tar
│   ├── kube-apiserver-amd64_v1.11.3.tar
│   ├── kube-controller-manager-amd64_v1.11.3.tar
│   ├── kube-proxy-amd64_v1.11.3.tar
│   ├── kubernetes-dashboard-amd64_v1.10.0.tar
│   ├── kube-scheduler-amd64_v1.11.3.tar
│   ├── nginx-ingress-controller_0.17.1.tar
│   ├── pause-amd64_3.1.tar
│   └── tiller_v2.9.1.tar
├── LICENSE
├── nohup.out
├── README.md
└── shell
    ├── auto-deploy.sh
    ├── del-current.sh
    ├── del-old.sh
    ├── install-lb.sh
    └── pull-gcr.sh

26 directories, 86 files
```

#### 2.4.2 设置环境

设置服务器环境，所有节点上都需要执行

```bash
curl -L http://172.30.80.88:8000/shell/auto-deploy.sh | bash -s env
```

安装ssh curl ntp等基本工具、设置ntp、关闭swap、更新系统、安装docker、安装kubelet/kubeadm/kubectl等。

#### 2.4.3 设置etcd集群

所有master节点上都需要执行

```bash
curl -L http://172.30.80.88:8000/shell/auto-deploy.sh | bash -s etcd --api-advertise-addresses=172.30.80.30 --etcd-endpoints=https://172.30.80.31:2379,https://172.30.80.32:2379,https://172.30.80.33:2379
```

必须在部署 master 节点之前完成。

```bash
sed -i s/"--initial-cluster-state new "/"--initial-cluster-state existing "/g /etc/systemd/system/etcd.service
systemctl daemon-reload && systemctl restart etcd
```

#### 2.4.4 部署第一个master节点

在节点master1上执行脚本

```bash
curl -L http://172.30.80.88:8000/shell/auto-deploy.sh | bash -s master --api-advertise-addresses=172.30.80.30 --etcd-endpoints=https://172.30.80.31:2379,https://172.30.80.32:2379,https://172.30.80.33:2379
```

- 172.30.80.88:8000 是http-server
- --api-advertise-addresses 是VIP地址
- --etcd-endpoints 是etcd集群的地址

记录下你的 token 输出，node节点加入集群时需要使用该token。

(k8s v1.11以后已默认使用 CoreDNS )

~~用 CoreDNS 替换 kube-dns~~ 

```bash
$ cd coredns && ./deploy.sh | kubectl apply -f -
$ kubectl delete --namespace=kube-system deployment kube-dns
```

~~或者集群安装后，使用~~ 

```bash
kubeadm upgrade apply <version> --feature-gates=CoreDNS=true
```

#### 2.4.5 部署其它的master节点

这里需要分别设置两个节点与第一个master的ssh互信。然后分别在两个节点上执行脚本

```bash
curl -L http://172.30.80.88:8000/shell/auto-deploy.sh | bash -s replica --api-advertise-addresses=172.30.80.30 --etcd-endpoints=https://172.30.80.31:2379,https://172.30.80.32:2379,https://172.30.80.33:2379
```

上面步骤完成之后，就实现了master节点的高可用。

#### 2.4.6 部署node

在每个node上分别执行脚本

```bash
curl -L http://172.30.80.88:8000/shell/auto-deploy.sh | bash -s join 172.30.80.30:6443 --token <token> --discovery-token-ca-cert-hash sha256:<sha256>
```

这里的`<token>`、`<sha256>`就是部署master完成后记录下的token

```bash
--token yugl4s.pfeea8zuzjb4qxqs --discovery-token-ca-cert-hash sha256:830f7dc338b46eb9c65fecd292a7fcf0350c089f2b2a9e4a109d72594e63e69b
```

#### 2.4.7 更新配置

后续加固见 2.6  等章节的内容。

上述步骤完成后，应该就能得到了一个完整的高可用集群。

### 2.5 部署 UI

#### 2.5.1 Kubernetes Dashboard

- 创建CA证书

```bash
openssl genrsa -des3 -out ca.key 2048
openssl rsa -in ca.key -out ca.key 
#openssl req -x509 -new -key ca.key -days 10000 -out ca.csr -subj "/CN=kube-ca"
openssl req -x509 -new -key ca.key -days 10000 -out ca.crt -subj "/CN=kube-ca"

cp /etc/ssl/openssl.cnf . #增加
openssl genrsa -des3 -out dashboard.key 2048
openssl req -new -key dashboard.key -out dashboard.csr -subj "/CN=kube-dashboard" -config openssl.cnf
openssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 365 -extensions v3_req -extfile openssl.cnf

```

创建secret

```bash
kubectl create secret generic kubernetes-dashboard-certs --from-file=. -n kube-system
```



- 修改配置文件

kubernetes-dashboard是可选组件，因为，实在不好用，功能太弱了。
建议在部署master时一起把kubernetes-dashboard部署了，不然在node节点加入集群后，kubernetes-dashboard会被kube-scheduler调度node节点上，这样跟kube-apiserver通信需要额外配置。

```bash
wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml
```

如果需要让外面访问需要修改这个yaml文件端口类型为NodePort，默认为clusterport外部访问不了

```bash
# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32666
  selector:
    k8s-app: kubernetes-dashboard
```

nodeport端口范围30000-32767，比如选择32666为映射端口，跟docker run -d xxx:xxx差不多

或者直接从http server下载已经修改好的配置文件

```bash
wget http://172.30.80.88:8000/config/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml
```

- load镜像

```bash
curl -L http://172.30.80.88:8000/images/kubernetes-dashboard-amd64_v1.10.0.tar -o /tmp/kubernetes-dashboard-amd64_v1.10.0.tar
docker load -i /tmp/kubernetes-dashboard-amd64_v1.10.0.tar
rm -rf /tmp/kubernetes-dashboard-amd64_v1.10.0.tar
```

- 创建kubernetes-dashboard

```bash
kubectl apply -f http://172.30.80.88:8000/config/dashboard/kubernetes-dashboard.yaml
kubectl apply -f http://172.30.80.88:8000/config/dashboard/dashboard-admin.yaml
kubectl apply -f http://172.30.80.88:8000/config/dashboard/dashboard-ingress.yaml

#kubectl delete -f http://172.30.80.88:8000/config/dashboard/dashboard-ingress.yaml
#kubectl delete -f http://172.30.80.88:8000/config/dashboard/dashboard-admin.yaml
#kubectl delete -f http://172.30.80.88:8000/config/dashboard/kubernetes-dashboard.yaml
```

- 若部署了`dashboard-admin.yaml`，则可以获取登陆`token`

```bash
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

```bash
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWtuNjJyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzYmU3YWVkOS1iYmIyLTExZTgtYWYzNC0wMDBjMjkyOGI4OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ib7mInIPTfjzDuzaAbmqfx1D7iCgBm3KRv6vkKZsyXsImDTNiRNaJDI4nBs6RdjK88EHOhclGCYrusXVx67WXNdx2UXhLOwxSSRsr05o-nJjL-m-KTYpNu3gF6zZJJeXWGTigvAJjn3UPAMtUYB1hidtKkJyJ61yw5h5jYxLL9dBiqXWcbYqSgm0VLdwChf8SpnNuOGbd8sRBj5_IIkTF6o5BQj-wtYiO2AX7wOG7DLyGJtENjJEdmUUT_eqqezUY14wiSACkhuMKXKYbhSaU0EBR6mJVz0GZCutNFf2CAZusSbSGNan8ROdhz2LldeMXK_f4IpmY4x2x7vQLSS1-Q
```

- 默认验证方式有kubeconfig和token，这里我们使用basicauth的方式进行apiserver的验证
  创建/etc/kubernetes/manifests/pki/basic_auth_file 用于存放用户名和密码。basic_auth_file文件格式为user,password,userid

  ```bash
  echo 'admin,admin,2' > /etc/kubernetes/pki/basic_auth_file
  ```

  给kube-apiserver添加basic_auth验证

  ```bash
  grep 'auth' /etc/kubernetes/manifests/kube-apiserver.yaml
      - --enable-bootstrap-token-auth=true
      - --authorization-mode=Node,RBAC
      - --basic_auth_file=/etc/kubernetes/pki/basic_auth_file 
  systemctl dameon-reload && systemctl restart kubectl
  #kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml  
  ```

- k8s1.6后版本都采用RBAC授权模型。默认情况下cluster-admin是拥有全部权限的，将admin和cluster-admin角色进行clusterrolebinding绑定，这样admin就有cluster-admin的权限。

  ```bash
  kubectl create clusterrolebinding  \
  login-on-dashboard-with-cluster-admin  \
  --clusterrole=cluster-admin --user=admin
  
  #kubectl get clusterrolebinding/login-on-dashboard-with-cluster-admin -o yaml
  #curl --insecure https://vm1:6443 -basic -u admin:admin
  
  ```

- 验证访问

```bash
kubectl describe svc kubernetes-dashboard -n kube-system
#kubectl describe ingress
```

```bash
$ kubectl proxy
# Now access Dashboard at:
# http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
```

- helm安装dashboard

```bash
helm install stable/kubernetes-dashboard --name dashboard --namespace kube-system -f myvalues.yaml

kubectl apply -f http://172.30.80.88:8000/config/cert-manager/cert-dashboard.yaml
```



#### 2.5.2 Weave Scope

```bash
kubectl apply -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

This downloads a recent Scope image from Dockerhub and launches a probe onto every node as well as a single Scope app. Once launched, Scope doesn’t require any other configuration.

Allowable parameters for the launcher URL:

- `v` - Weave Scope version or tag, e.g. `latest` current release is the default
- `k8s-service-type` - Kubernetes service type (for running Scope in Standalone mode), can be either `LoadBalancer` or `NodePort`, by default this is unspecified (only internal access)

**Open Scope in Your Browser**

```bash
kubectl port-forward -n weave "$(kubectl get -n weave pod --selector=weave-scope-component=app -o jsonpath='{.items..metadata.name}')" 4040
```

The URL is: http://localhost:4040.

```bash
kubectl delete -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

```bash
curl -sL -o scope.yaml https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')
```

#### 2.5.3 Kubebox



### 2.6 其它配置

关于DNS

```bash
kubectl scale deployment --replicas 3 kube-dns -n kube-system
# or coredns
```

需配置coredns (或者kube-dns) 的 subdomains

```bash
proxy ssii.com 172.30.80.89 172.30.80.88 {
  policy round_robin
}
proxy . /etc/resolv.conf {
  except ssii.com
}
```

设置master节点的label traint

```bash
kubectl label node `hostname` node-type=master

kubectl taint node `hostname` node-role.kubernetes.io/master="":NoSchedule
kubectl taint nodes `hostname` master=yes:NoSchedule
```

### 2.7 部署管理工具

注意：因为GWF网络问题，helm draft的安装有问题

#### 2.7.0 helm charts稳定可访问源

```bash
stable http://mirror.azure.cn/kubernetes/charts/
incubator http://mirror.azure.cn/kubernetes/charts-incubator/
svc-cat http://mirror.azure.cn/kubernetes/svc-catalog-charts/
```



#### 2.7.1 Helm

首先必须创建如下角色，否则操作中会报错

```bash
kubectl create clusterrolebinding add-on-cluster-admin \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:default
kubectl create clusterrolebinding tiller-cluster-admin \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:default
```

下载

```bash
$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh

# or simply
curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash
```

将下载的helm解压并运行

```bash
helm init
```

```bash
# 仅安装客户端
export HELM_HOME=/some/path
helm init --client-only
```

升级tiller

```bash
$ export TILLER_TAG=v2.10.0        # Or whatever version you want
$ kubectl --namespace=kube-system set image deployments/tiller-deploy tiller=gcr.io/kubernetes-helm/tiller:$TILLER_TAG
```

建议在生产环境中，基于 RBAC 安装 helm

helm 安装 dashboard

```bash
helm install stable/kubernetes-dashboard --name my-release -f values.yaml
```

helm 常用命令

```bash
# repo 操作
helm repo add <chart-repo-name> <chart-repo-url>
helm repo update
# chart 操作
helm list
helm dep[endency] up[date] <name>
helm inspect <name>
helm lint <name>
helm install --name <name> <repo>/<name> -f myvalues.yaml
helm del [--purge] <name>
helm upgrade --reuse-values <name> <chart-repo-name>|<chart-repo-url>

```

安装helm push插件

```bash
helm plugin install https://github.com/chartmuseum/helm-push
```

- Pushing a directory

Point to a directory containing a valid `Chart.yaml` and the chart will be packaged and uploaded:

```
$ cat mychart/Chart.yaml
name: mychart
version: 0.3.2
```

```
$ helm push mychart/ chartmuseum
Pushing mychart-0.3.2.tgz to chartmuseum...
Done.
```

- Pushing with a custom version

The `--version` flag can be provided, which will push the package with a custom version.

Here is an example using the last git commit id as the version:

```
$ helm push mychart/ --version="$(git log -1 --pretty=format:%h)" chartmuseum
Pushing mychart-5abbbf28.tgz to chartmuseum...
Done.
```

If you want to enable something like `--version="latest"`, which you intend to push regularly, you will need to run your ChartMuseum server with `ALLOW_OVERWRITE=true`.

- Push .tgz package

This workflow does not require the use of `helm package`, but pushing .tgzs is still suppported:

```
$ helm push mychart-0.3.2.tgz chartmuseum
Pushing mychart-0.3.2.tgz to chartmuseum...
Done.
```

- Force push

If your ChartMuseum install is configured with `ALLOW_OVERWRITE=true`, chart versions will be automatically overwritten upon re-upload.

Otherwise, unless your install is configured with `DISABLE_FORCE_OVERWRITE=true` (ChartMuseum > v0.7.1), you can use the `--force`/`-f` option to to force an upload:

```
$ helm push --force mychart-0.3.2.tgz chartmuseum
Pushing mychart-0.3.2.tgz to chartmuseum...
Done.
```

- Pushing directly to URL

If the second argument provided resembles a URL, you are not required to add the repo prior to push:

```
$ helm push mychart-0.3.2.tgz http://localhost:8080
Pushing mychart-0.3.2.tgz to http://localhost:8080...
Done.
```

#### 2.7.2 Draft

```bash
$ draft init
# if using local cluster minikube
#$ eval $(minikube docker-env)
```



#### 2.7.3 Skaffold

```bash
curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-darwin-amd64 && chmod +x skaffold && sudo mv skaffold /usr/local/bin
```



#### 2.7.4 Apollo

```bash
# need docker-compose
git clone https://github.com/logzio/apollo.git
cd apollo/examples
docker-compose up -d

# Apollo should run at http://localhost:30001/
```



#### 2.7.5 Keel

```bash
# helm upgrade --install keel stable/keel --set helmProvider.enabled="true"
git clone https://github.com/keel-hq/keel.git && cd keel
helm upgrade --install keel --namespace=kube-system ./chart/keel/ --set helmProvider.enabled="true" --set rbac.enabled="true"
```



#### 2.7.6 kube-shell

```bash
pip install kube-shell
```



#### 2.7.7 kail

```bash
$ brew tap boz/repo
$ brew install boz/repo/kail
```

#### 2.7.8 Kaniko

```bash
kubectl create secret generic kaniko-secret --from-file=./.docker/config.json
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kaniko
spec:
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:latest
    args: ["--dockerfile=<path to Dockerfile within the build context>",
            "--context=gs://<GCS bucket>/<path to .tar.gz>",
            "--destination=<gcr.io/$PROJECT/$IMAGE:$TAG>"]
    volumeMounts:
      - name: kaniko-secret
        mountPath: /secret
    env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/kaniko-secret.json
  restartPolicy: Never
  volumes:
    - name: kaniko-secret
      secret:
        secretName: kaniko-secret
```

### 2.8 升级集群

#### 2.8.1 准备升级文件和镜像

首先下载符合升级版本的 `kubeadm`:

```bash
# Use the latest stable release or manually specify a
# released Kubernetes version
export VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt) 
export ARCH=amd64 # or: arm, arm64, ppc64le, s390x
curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > /tmp/kubeadm
chmod a+rx /tmp/kubeadm
```

将 `kubeadm` 拷贝到 primary master 的 `/tmp` . 可以执行如下命令查看可用的升级版本:

```bash
/tmp/kubeadm upgrade plan
```

结果显示如下:

```bash
Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.9.0    v1.9.2
Controller Manager   v1.9.0    v1.9.2
Scheduler            v1.9.0    v1.9.2
Kube Proxy           v1.9.0    v1.9.2
Kube DNS             1.14.5    1.14.7
Etcd                 3.2.7     3.1.11

```

> **Caution:** Currently the only supported  configuration for kubeadm HA clusters requires the use of an externally  managed etcd cluster. Upgrading etcd is not supported as a part of the  upgrade. If necessary you will have to upgrade the etcd cluster  according to [etcd’s upgrade instructions](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/), which is beyond the scope of these instructions.

#### 2.8.2 升级etcd

1. Check upgrade requirements

2. Stop the existing etcd process
3. Drop-in etcd v3.2 binary and start the new etcd process
4. Repeat step 2 to step 3 for all other members
5. Finish

#### 2.8.3 升级k8s组件

下面的升级过程必须在每个 master node 上顺序执行.

Before initiating the upgrade with `kubeadm` `configmap/kubeadm-config`  needs to be modified for the current master host. Replace any hard  reference to a master host name with the current master hosts’ name:

```bash
kubectl get configmap -n kube-system kubeadm-config -o yaml >/tmp/kubeadm-config-cm.yaml
sed -i 's/^\([ \t]*nodeName:\).*/\1 <CURRENT-MASTER-NAME>/' /tmp/kubeadm-config-cm.yaml
kubectl apply -f /tmp/kubeadm-config-cm.yaml --force
```

Now the upgrade process can start. Use the target version determined  in the preparation step and run the following command (press “y” when  prompted):

```bash
/tmp/kubeadm upgrade apply v<YOUR-CHOSEN-VERSION-HERE>
```

If the operation was successful you’ll get a message like this:

```bash
[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.9.2". Enjoy!
```

To upgrade the cluster with CoreDNS as the default internal DNS, invoke `kubeadm upgrade apply` with the `--feature-gates=CoreDNS=true` flag.

Next, manually upgrade your CNI provider

Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow. Check the [addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/) page to find your CNI provider and see if there are additional upgrade steps necessary.

> **Note:** The `kubeadm upgrade apply`  step has been known to fail when run initially on the secondary masters  (timed out waiting for the restarted static pods to come up). It should  succeed if retried after a minute or two.

**特别注意：**

在升级第二个master之前，在第一个master上执行：

```bash
kubectl drain `hostname` --ignore-daemonsets
```

升级 k8s 工具之后，再在第一个master上执行：

```bash
kubectl uncordon `hostname`
```



#### 2.8.4 升级k8s工具

At this point all the static pod manifests in your cluster, for  example API Server, Controller Manager, Scheduler, Kube Proxy have been  upgraded, however the base software, for example `kubelet`, `kubectl`, `kubeadm`  installed on your nodes’ OS are still of the old version. For upgrading  the base software packages we will upgrade them and restart services on  all nodes one by one:

```bash
# use your distro's package manager, e.g. 'yum' on RH-based systems
# for the versions stick to kubeadm's output (see above)
yum install -y kubelet-<NEW-K8S-VERSION> kubectl-<NEW-K8S-VERSION> kubeadm-<NEW-K8S-VERSION> kubernetes-cni-<NEW-CNI-VERSION>
systemctl restart kubelet
```

In this example an *rpm*-based system is assumed and `yum` is used for installing the upgraded software. On *deb*-based systems it will be `apt-get update` and then `apt-get install <PACKAGE>=<NEW-K8S-VERSION>` for all packages.

Now the new version of the `kubelet` should be running on the host. Verify this using the following command on the respective host:

```bash
systemctl status kubelet
```

Verify that the upgraded node is available again by executing the following from wherever you run `kubectl` commands:

```bash
kubectl get nodes
```

If the `STATUS` column of the above command shows `Ready` for the upgraded host, you can continue (you may have to repeat this for a couple of time before the node gets `Ready`).

#### 2.8.5 升级失败后的回退

If the upgrade fails the situation afterwards depends on the phase in which things went wrong:

1. If `/tmp/kubeadm upgrade apply` failed to upgrade the  cluster it will try to perform a rollback. Hence if that happened on the  first master, chances are pretty good that the cluster is still intact.

You can run `/tmp/kubeadm upgrade apply` again as it is idempotent and should eventually make sure the actual state is the desired state you are declaring. You can use `/tmp/kubeadm upgrade apply` to change a running cluster with `x.x.x --> x.x.x` with `--force`, which can be used to recover from a bad state.

1. If `/tmp/kubeadm upgrade apply` on one of the secondary  masters failed you still have a working, upgraded cluster, but with the  secondary masters in a somewhat undefined condition. You will have to  find out what went wrong and join the secondaries manually. As mentioned  above, sometimes upgrading one of the secondary masters fails waiting  for the restarted static pods first, but succeeds when the operation is  simply repeated after a little pause of one or two minutes.

### 2.9 RBAC配置



## 3. 运行时和工具的安装

### 3.1 配套工具安装配置

#### 3.1.1 安全管理

##### CA: cert-manager

```bash
# Install the CustomResourceDefinition resources separately
kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml

# Create the namespace for cert-manager
kubectl create namespace cert-manager

# Label the cert-manager namespace to disable resource validation
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install \
  --name cert-manager \
  --namespace cert-manager \
  --version v0.8.1 \
  --set nodeSelector."node-type"=master \
  jetstack/cert-manager
  
helm install \
--name cert-manager \
--namespace kube-system mirror/cert-manager \
--set nodeSelector."node-type"=master,tolerations[0].key=node-role.kubernetes.io/master,tolerations[0].operator=Equal,tolerations[0].value="",tolerations[0].effect=NoSchedule
```

ClusterIsurrer/Isurrer

```bash
kubectl apply -f http://172.30.80.88:8000/config/cert-manager/issuer.yaml
```

Certificates

```bash
kubectl apply -f http://172.30.80.88:8000/config/cert-manager/cert-ingress.yaml
```

创建secret

首先需完成“2.1.8部署CA”，然后

```bash
kubectl create secret tls ca-key-pair \
   --cert=ca.cert.pem \
   --key=ca.key.pem \
   --namespace=cert-manager
```

```bash
$ kubectl describe certificate jx-ingress-certs
$ kubectl get secret jx-ingress-certs -o yaml
```

##### NetworkPolicy : Trireme

requied: IPTables

```bash
git clone https://github.com/aporeto-inc/trireme-kubernetes.git
cd trireme-kubernetes/deployment

# Create the ConfigMap
kubectl create -f trireme-config-cm.yaml

# Optionally, deploy the Trireme-Statistics bundle
kubectl create -f statistics/

# Create a dummy self-signed Certificate Authority (CA) for Trireme-CSR (the identity service) and add it as a Kubernetes secret
go get -u github.com/aporeto-inc/tg
./gen_pki_ca.sh

# Finally, deploy Trireme-CSR and Trireme-Kubernetes
kubectl create -f trireme/
```



#### 3.1.2 共享存储

##### NFS: nfs-client

```bash
#原始repo
git clone https://github.com/kubernetes-incubator/external-storage.git
cd nfs-client
```

修改deployment文件并部署 deploy/deployment.yaml 

```bash
kubectl create -f http://172.30.80.88:8000/config/nfs-client/deployment.yaml
```

修改StorageClass文件并部署 deploy/class.yaml 

```bash
kubectl create -f http://172.30.80.88:8000/config/nfs-client/class.yaml
```

如果启用了RBAC需要执行如下的命令来授权 

```bash
kubectl create -f http://172.30.80.88:8000/config/nfs-client/rbac.yaml
```

设置 nfs-client StorageClass 为Kubernetes的默认存储后端 

```bash
kubectl patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

##### GlusterFS



#### 3.1.3 MetalLB

install using manifests

```bash
kubectl apply -f http://172.30.80.88:8000/config/metallb/metallb.yaml
kubectl apply -f http://172.30.80.88:8000/config/metallb/metallb-config.yaml
```



install using helm

```bash
# install with `ARP Mode` - address range MUST NOT in k8s cluster
helm install --name metallb \
  --set arpAddresses=172.30.80.100-172.30.80.150 \
  stable/metallb
```



#### 3.1.4 容器管理

##### 私有镜像harbor



##### 容器分发dragonfly



##### Chartmuseum

```bash
helm install \
--name chartmuseum \
--namespace env stable/chartmuseum \
--set \
persistence.enabled=true,\
env.open.DISABLE_API=false,\
env.secret.BASIC_AUTH_USER='admin',\
env.secret.BASIC_AUTH_PASS='admin123',\
env.open.AUTH_ANONYMOUS_GET=true,\
service.type=LoadBalancer,\
nodeSelector."node-type"=master,\
tolerations[0].key=node-role.kubernetes.io/master,\
tolerations[0].operator=Equal,\
tolerations[0].value="",\
tolerations[0].effect=NoSchedule
```

或者

```bash
helm install --name chartmuseum --namespace env -f config/chartmuseum/myvalues.yaml stable/chartmuseum
```

~~创建CA~~

```bash
openssl genrsa -out chart.key 2048
openssl req -new -key chart.key -out chart.csr -subj "/CN=chartmuseum.ssii.com" -config openssl_root.cnf
openssl x509 -req -in chart.csr -CA ca.cert.pem -CAkey ca.key.pem -CAcreateserial -out chart.crt -days 3650 -extensions v3_req -extfile openssl_root.cnf
```

添加repo到helm

```bash
helm repo add --username admin --password admin123 chartmuseum https://chartmuseum.ssii.com
```

添加其它的helm repo

```bash
# helm repo list
NAME     	URL                                                   
local    	http://127.0.0.1:8879/charts                          
monocular	https://helm.github.io/monocular                      
aliyuncs 	https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
stable   	https://kubernetes-charts.storage.googleapis.com
mirror		http://172.30.80.33:8080/
bitnami		https://charts.bitnami.com/bitnami
```

创建 chart mirror

```bash
git clone https://github.com/helm/chartmuseum.git

# on Linux
curl -LO https://s3.amazonaws.com/chartmuseum/release/latest/bin/linux/amd64/chartmuseum
chmod +x ./chartmuseum
mv ./chartmuseum /usr/local/bin

scripts/mirror_k8s_repos.sh
chartmuseum --debug --port=8080 --storage="local" --storage-local-rootdir="./mirror"
```

创建用于 kubeapps 的 secret

```bash
kubectl create -n kubeapps secret generic chartmuseum-crt --from-file=./certs/chartmuseum.crt
```

##### Monocular

```bash
# #Prerequisites
# - Helm and Tiller installed
# - Nginx Ingress controller
#     - Install with Helm: helm install stable/nginx-ingress
#     - Minikube/Kubeadm: helm install stable/nginx-ingress --set controller.hostNetwork=true

$ helm repo add monocular https://helm.github.io/monocular
$ helm install --name monocular --namespace env monocular/monocular -f myvalues.yaml 
```

##### Kubeapps

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install --name kubeapps --namespace env bitnami/kubeapps -f myvalues.yaml
```

```bash
# create a k8s API token
kubectl create serviceaccount kubeapps-operator
kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator

kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='{.secrets[].name}') -o jsonpath='{.data.token}' | base64 --decode && echo
```

```bash
# create ingress for kubeapps service
kubectl apply -f kubeapps/ingress.yaml
```

- Installing the Chart

To install the chart with the release name `kubeapps`:

```bash
$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm install --name kubeapps --namespace env bitnami/kubeapps
```

> **IMPORTANT** This assumes an insecure Helm installation, which is not recommended in production. See [the documentation to learn how to secure Helm and Kubeapps in production](https://github.com/kubeapps/kubeapps/blob/master/docs/user/securing-kubeapps.md).

The command deploys Kubeapps on the Kubernetes cluster in the `kubeapps` namespace. The [configuration](https://github.com/kubeapps/kubeapps/blob/master/chart/kubeapps/README.md#configuration) section lists the parameters that can be configured during installation.

> **Caveat**: Only one Kubeapps installation is supported per namespace

> **Tip**: List all releases using `helm list`

- Add Chart Repos

```bash
initialRepos:
  # - name: stable
  #   url: https://kubernetes-charts.storage.googleapis.com
  # - name: incubator
  #   url: https://kubernetes-charts-incubator.storage.googleapis.com
  # - name: svc-cat
  #   url: https://svc-catalog-charts.storage.googleapis.com
  - name: mirror
    url: http://172.30.80.33:8080
  - name: bitnami
    url: https://charts.bitnami.com/bitnami
  - name: chartmuseum
    url: http://chartmuseum.default.svc.cluster.local:8080
    # url: https://chartmuseum.ssii.com
```

- Upgrading Kubeapps

To upgrade Kubeapps, first ensure you have the updated your local chart repository cache:

```
$ helm repo update
```

Now upgrade Kubeapps:

```bash
$ export RELEASE_NAME=kubeapps
$ export NAMESPACE=env
$ export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace $NAMESPACE $RELEASE_NAME-mongodb -o jsonpath="{.data.mongodb-root-password}" | base64 --decode)
$ helm upgrade $RELEASE_NAME bitnami/kubeapps --set mongodb.mongodbRootPassword=$MONGODB_ROOT_PASSWORD
```

The above commands ensure the MongoDB password is set to the existing password and not regenerated by the chart.

- Uninstalling Kubeapps

To uninstall/delete the `kubeapps` deployment:

```bash
$ helm delete --purge kubeapps
$ # Optional: Only if there are no more instances of Kubeapps
$ kubectl delete crd apprepositories.kubeapps.com
```

The first command removes most of the Kubernetes components  associated with the chart and deletes the release. After that, if there  are no more instances of Kubeapps in the cluster you can manually delete  the `apprepositories.kubeapps.com` CRD used by Kubeapps that is shared for the entire cluster.

> **NOTE**: If you delete the CRD for `apprepositories.kubeapps.com` it will delete the repositories for **all** the installed instances of `kubeapps`. This will break existing installations of `kubeapps` if they exist.

##### Service Catalog

- Tiller Permissions

Tiller is the in-cluster server component of Helm. By default, `helm init` installs the Tiller pod into the `kube-system` namespace, and configures Tiller to use the `default` service account.

Tiller will need to be configured with `cluster-admin` access to properly install Service Catalog:

```bash
kubectl create clusterrolebinding tiller-cluster-admin \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:default
```

- Helm Repository Setup

Service Catalog is easily installed via a [Helm chart](https://github.com/kubernetes/helm/blob/master/docs/charts.md).

This chart is located in a [chart repository](https://github.com/kubernetes/helm/blob/master/docs/chart_repository.md) just for Service Catalog. Add this repository to your local machine:

```bash
helm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com
```

Then, ensure that the repository was successfully added:

```bash
helm search service-catalog
```

You should see the following output:

```
NAME           	VERSION	DESCRIPTION
svc-cat/catalog	x,y.z  	service-catalog API server and controller-manag...
```

- Install Service Catalog

Now that your cluster and Helm are configured properly, installing Service Catalog is simple:

```bash
helm install svc-cat/catalog --name catalog --namespace catalog
```

- Installing the Service Catalog CLI

Follow the appropriate instructions for your operating system to install svcat. The binary can be used by itself, or as a kubectl plugin.

The snippets below install the latest version of svcat:

Latest release: [download.svcat.sh/cli/latest](https://download.svcat.sh/cli/latest)

MacOS with Homebrew

```bash
brew update
brew install kubernetes-service-catalog-client
```

MacOS

```bash
curl -sLO https://download.svcat.sh/cli/latest/darwin/amd64/svcat
chmod +x ./svcat
mv ./svcat /usr/local/bin/
svcat version --client
```

Linux

```bash
curl -sLO https://download.svcat.sh/cli/latest/linux/amd64/svcat
chmod +x ./svcat
mv ./svcat /usr/local/bin/
svcat version --client
```

Windows

The snippet below adds a directory to your PATH for the current session only. You will need to find a permanent location for it and add it to your PATH.

```bash
iwr 'https://download.svcat.sh/cli/latest/windows/amd64/svcat.exe' -UseBasicParsing -OutFile svcat.exe
mkdir -f ~\bin
$env:PATH += ";${pwd}\bin"
svcat version --client
```

Manual

1. Download the appropriate binary for your operating system: 
   - macOS: [download.svcat.sh/cli/latest/darwin/amd64/svcat](https://download.svcat.sh/cli/latest/darwin/amd64/svcat)
   - Windows: [download.svcat.sh/cli/latest/windows/amd64/svcat.exe](https://download.svcat.sh/cli/latest/windows/amd64/svcat.exe)
   - Linux: [download.svcat.sh/cli/latest/linux/amd64/svcat](https://download.svcat.sh/cli/latest/linux/amd64/svcat)
2. Make the binary executable.
3. Move the binary to a directory on your PATH.

- Plugin

To use svcat as a plugin, run the following command after downloading:

```bash
$ ./svcat install plugin
Plugin has been installed to ~/.kube/plugins/svcat. Run kubectl plugin svcat --help for help using the plugin.
```

When operating as a plugin, the commands are the same with the addition of the global kubectl configuration flags. One exception is that boolean flags aren't supported when running in plugin mode, so instead of using `--flag` you must specify a value `--flag=true`.

##### Automation Broker

- Prerequisites:
  - Install the `helm` binary.
  - Clone the broker's GitHub repository

  ```bash
  git clone https://github.com/openshift/ansible-service-broker
  ```

- Install

Run the following from the root of the cloned git repository.

```bash
#!/bin/env bash

# Adjust the version to your liking. Follow installation docs
# at https://github.com/kubernetes/minikube.
#minikube start --bootstrapper kubeadm --kubernetes-version v1.9.4

# Install helm and tiller. See documentation for obtaining the helm
# binary. https://docs.helm.sh/using_helm/#install-helm
#helm init

# Wait until tiller is ready before moving on
#until kubectl get pods -n kube-system -l name=tiller | grep 1/1; do sleep 1; done

#kubectl create clusterrolebinding tiller-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default

# Adds the chart repository for the service catalog
helm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com

# Installs the service catalog
helm install svc-cat/catalog --name catalog --namespace catalog

# Wait until the catalog is ready before moving on
until kubectl get pods -n catalog -l app=catalog-catalog-apiserver | grep 2/2; do sleep 1; done
until kubectl get pods -n catalog -l app=catalog-catalog-controller-manager | grep 1/1; do sleep 1; done

./scripts/run_latest_k8s_build.sh
```

- Use

Once everything is installed, you can interact with the service catalog using the `svcat` command. Learn how to install and use it here(https://github.com/kubernetes-incubator/service-catalog/tree/master/cmd/svcat).

##### Athens (Go mod proxy)

```bash
helm repo add gomods https://athens.blob.core.windows.net/charts
helm repo update
helm install gomods/athens-proxy -n athens --namespace athens --set replicaCount=3
```



#### 3.1.5 网关

##### ingress-nginx

~~首先创建CA~~

```bash
# 生成 CA 自签证书
mkdir cert && cd cert
openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=ingress-ca"

# 编辑 openssl 配置
cp /etc/ssl/openssl.cnf .
vim openssl.cnf

# 主要修改如下
[req]
req_extensions = v3_req # 这行默认注释关着的 把注释删掉
# 下面配置是新增的
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = k8s.rdd.ssii.com

# 生成证书
openssl genrsa -out ingress-key.pem 2048
openssl req -new -key ingress-key.pem -out ingress.csr -subj "/CN=k8s.rdd.ssii.com" -config openssl.cnf
openssl x509 -req -in ingress.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ingress.pem -days 365 -extensions v3_req -extfile openssl.cnf
```

~~创建secret~~

```bash
kubectl create secret tls ingress-secret -n kube-system --key cert/ingress-key.pem --cert cert/ingress.pem
```

~~部署service时，需要再ingress配置文件中指定~~

```bash
tls:
  - hosts:
    - k8s.rdd.ssii.com
    secretName: ingress-secret
```

- Install commands

```bash
kubectl apply -f http://172.30.80.88:8000/config/ingress/ingress-nginx.yaml
kubectl apply -f http://172.30.80.88:8000/config/ingress/baremetal-service.yaml
```

- verify

```bash
kubectl get pods --all-namespaces -l app=ingress-nginx --watch

POD_NAMESPACE=ingress-nginx
POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app=ingress-nginx -o jsonpath={.items[0].metadata.name})
kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version
```

- remove

```bash
kubectl delete -f http://172.30.80.88:8000/config/ingress/ingress-nginx.yaml
```

- helm  安装

```bash
helm install stable/nginx-ingress --name my-release -f myvalues.yaml
```

##### external-dns

选择powerdns作为后端

- Deployment

Deploying external DNS for PowerDNS is actually nearly identical to deploying it for other providers. This is what a sample `deployment.yaml` looks like:

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      # Only use if you're also using RBAC
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:latest
        args:
        - --source=service
        - --source=ingress
        - --provider=pdns
        - --pdns-server=http://172.30.80.89:8081
        - --pdns-api-key=pdnsapi
        - --txt-owner-id=external-dns
        - --log-level=debug
        - --interval=30s
```

- RBAC

If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: default
```

- Testing and Verification

**Important!**: Remember to change `example.com` with your own domain throughout the following text.

Spin up a simple "Hello World" HTTP server with the following spec (`kubectl apply -f`):

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: echo
spec:
  template:
    metadata:
      labels:
        app: echo
    spec:
      containers:
      - image: hashicorp/http-echo
        name: echo
        ports:
        - containerPort: 5678
        args:
          - -text="Hello World"
---
apiVersion: v1
kind: Service
metadata:
  name: echo
  annotations:
    external-dns.alpha.kubernetes.io/hostname: echo.ssii.com
spec:
  selector:
    app: echo
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5678
```

**Important!**: Don't run dig, nslookup or similar immediately (until you've confirmed the record exists). You'll get hit by [negative DNS caching](https://tools.ietf.org/html/rfc2308), which is hard to flush.

Run the following to make sure everything is in order:

```bash
$ kubectl get services echo
$ kubectl get endpoints echo
```

Make sure everything looks correct, i.e the service is defined and recieves a public IP, and that the endpoint also has a pod IP.

Once that's done, wait about 30s-1m (interval for external-dns to kick in), then do:

```bash
$ curl -H "X-API-Key: pdnsapi" http://172.30.80.89:8081/api/v1/servers/localhost/zones/ssii.com. | jq '.rrsets[] | select(.name | contains("echo"))'
```

Once the API shows the record correctly, you can double check your record using:

```bash
$ dig @172.30.80.89 echo.ssii.com.
```

#### 3.1.6 日志监控和消息系统

##### heapster

install using helm

```bash
helm install stable-mirror/heapster --name heapster --namespace kube-system -f config/heapster/myvalues.yaml
```

install using manifest

```bash
kubectl apply -f http://172.30.80.88:8000/config/heapster/heapster.yaml
kubectl apply -f http://172.30.80.88:8000/config/heapster/influxdb.yaml
kubectl apply -f http://172.30.80.88:8000/config/heapster/grafana.yaml
```

Edit the Heapster RBAC role and add the get permission on the nodes statistic at the end.

```bash
$ kubectl edit clusterrole system:heapster
...
- apiGroups:
  - ""
  resources:
  - nodes/stats
  verbs:
  - get
```

##### ELK+B/EFK

- fluentd-elasticsearch

```bash
helm repo add kiwigrid https://kiwigrid.github.io
helm install --name fluentd -f values.yaml kiwigrid/fluentd-elasticsearch
```

- elasticsearch

```bash
helm install --name elasticsearch stable/elasticsearch -f values.yaml
```

- kibana

```bash
helm install stable/kibana --name kibana -f values.yaml
```



##### Prometheus



#### 3.1.7 开发运行时工具

##### Nexus

- helm安装

```bash
helm install --name nexus --namespace env stable/sonatype-nexus -f myvalues.yaml
```

默认的管理账号

```bash
username: admin
password: admin123
```

安装完毕建议修改账号密码

- 安装后配置LDAP

Connection

```bash
LDAP server address: ldap:ldap.ssii.com:389
Search base: dc=rdp,dc=dev
Authentication method: Simple Authentication
Username or DN: cn=admin,dc=rdp,dc=dev
```

User and group

```bash
Base DN: ou=users
User subtree: <checked>
Object Class: InetOrgPerson
User ID attribute: uid
real name attribute: cn(displayName)
Email attribute: mail
Password attribute: userPassword
Map LDAP groups as roles: <checked>
Group type: Static Groups
Group base DN: ou=realmRoles
Group object class: groupOfNames
Group ID attribute: cn
Group member attribute: member
Group member format: "uid=${username},ou=users,dc=rdp,dc=dev"
```

map ldap group to nexus Roles

- 配置软件库

Create a blob store to host proxied artifacts

```bash
Blob Stores -> Type:file Name:proxied Path:/nexus-data/blobs/proxied
```

Configure `nuget.org-proxy` to use the `proxied` blob store

```bash
# delete the nuget.org-proxy repository and re-create it
# Repository type: nuget (proxy)
Name: nuget.org-proxy
Online: <true>
Metadata query cache age: 3600
Remote storage: https://www.nuget.org/api/v2/
Auto blocking enabled: <true>
Maximun component age: 1440
Maximun metadata age: 1440
Blob store: proxied
Strict Content Type Validation: <true>
Not found cache enabled: <true>
Not found cache TTL: 1440
```

Configure `maven-central` proxy to use the `proxied` blob store, and add `maven-central` to the list of *Member repositories* of `maven-public`.

```bash
# delete the maven-central repository and re-create it
# Repository type: maven2 (proxy)
Name: maven-central
Online: <true>
Version policy: Release
Layout policy: Permissive
Remote storage: https://repo1.maven.org/maven2/
Auto blocking enabled: <true>
Maximun component age: -1
Maximun metadata age: 1440
Blob store: proxied
Strict Content Type Validation: <true>
Not found cache enabled: <true>
Not found cache TTL: 1440
```

Configure the `docker-hosted` repository:

```bash
# Repository type: docker (hosted)
Name: docker-hosted
HTTP: 5003
Blob store: default
Deployment policy: Allow redeploy
```

Configure the `pypi-all` repository group:

```bash
# Configuring pypi-proxied
# Repository type: pypi (proxy)
Name: pypi-proxied
Remote storage: https://pypi.python.org/
Auto blocking enabled: <true>
Maximun component age: 1440
Maximun metadata age: 1440
Blob store: proxied
Strict Content Type Validation: <true>
Not found cache enabled: <true>
Not found cache TTL: 1440
```

```bash
# Configuring pypi-hosted
# Repository type: pypi (hosted)
Name: pypi-hosted
Online: <true>
Blob store: default
Strict Content Type Validation: <true>
Deployment policy: Disable redeploy
```

```bash
# Configuring pypi-all
# Repository type: pypi (group)
Name: pypi-all
Online: <true>
Blob store: default
Strict Content Type Validation: <true>
# add Group -> Menbers: pypi-hosted pypi-proxied
```

Configure backup tasks and create backup scripts

- 配置 k8s 拉取私有镜像

```bash
echo -n "admin:admin123" | base64
YWRtaW46YWRtaW4xMjM=
```

```bash
mkdir ~/.docker
cat > ~/.docker/config.json << EOF
{
  "auths": {
    "containers.ssii.com": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
EOF
```

```bash
cat << EOF | base64
{
  "auths": {
    "containers.ssii.com": {
      "email": "admin@ssii.com",
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
EOF
# ewogICJhdXRocyI6IHsKICAgICJjb250YWluZXJzLnNzaWkuY29tIjogewogICAgICAiZW1haWwiOiAiYWRtaW5Ac3NpaS5jb20iLAogICAgICAiYXV0aCI6ICJZV1J0YVc0NllXUnRhVzR4TWpNPSIKICAgIH0KICB9Cn0K
```

```bash
cat > nexus-docker.yaml << EOF
apiVersion: v1
data:
 .dockercfg:
 ewogICJhdXRocyI6IHsKICAgICJjb250YWluZXJzLnNzaWkuY29tIjogewogICAgICAiZW1haWwiOiAiYWRtaW5Ac3NpaS5jb20iLAogICAgICAiYXV0aCI6ICJZV1J0YVc0NllXUnRhVzR4TWpNPSIKICAgIH0KICB9Cn0K
kind: Secret
metadata:
  name: nexus-docker
type: kubernetes.io/dockercfg
EOF

kubectl create -f nexus-docker.yaml
```

在k8s中使用

```bash
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    -
      image: containers.ssii.com/devops/docker-nexus:3.3.2
      name: my-container
  imagePullSecrets:
    -
      name: nexus-docker
```



##### Jenkins

```bash
helm install --name jenkins --namespace env stable/jenkins -f myvalues.yaml
```

安装后配置：

Configure the `Kubernetes plugin` in Jenkins to use the following Service Account name `jenkins` using the following steps:
​	Create a Jenkins credential of type Kubernetes service account with service account name `jenkins`
​	Under configure Jenkins -- Update the credentials config in the cloud section to use the service account credential you created in the step above.

##### GitLab

首先创建数据库

```bash
CREATE USER gitlab WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE gitlab OWNER gitlab;
GRANT ALL PRIVILEGES ON DATABASE gitlab TO gitlab;
```

生成secret

```bash
# gitlab_password
echo -n "p@ssw0rd"| base64
# gitlab_userpassword
echo -n "gitlab:p@ssw0rd"| base64

```

```bash
cat | kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: gitlab-postgresql-secret
  namespace: env
type: Opaque
data:
  gitlab_password: cEBzc3cwcmQ=
  gitlab_userpassword: Z2l0bGFiOnBAc3N3MHJk
EOF
```

helm安装

```bash
helm repo add gitlab https://charts.gitlab.io/
helm repo update
helm upgrade --install gitlab gitlab/gitlab \
  --namespace env --timeout 600 \
  --set global.hosts.domain=ssii.com \
  --set global.hosts.externalIP=172.30.80.100 \
  --set certmanager.install=false \
  --set global.ingress.configureCertmanager=false \
  --set global.ingress.tls.secretName=gitlab-ingress-certs \
  --set postgresql.install=false \
  --set global.psql.host=172.30.80.103 \
  --set global.psql.password.secret=gitlab-postgresql-secret \
  --set global.psql.password.key=gitlab-password \
  --set gitlab.migrations.image.repository=registry.gitlab.com/gitlab-org/build/cng/gitlab-rails-ce \
  --set gitlab.sidekiq.image.repository=registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ce \
  --set gitlab.unicorn.image.repository=registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ce \
  --set gitlab.unicorn.workhorse.image=registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ce \
  --set gitlab.task-runner.image.repository=registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ce
```

Initial login

root 账号登陆，密码可以如下获取：

```bash
kubectl get secret gitlab-gitlab-initial-root-password -ojsonpath={.data.password} | base64 --decode ; echo
```



##### Gitea

首先创建数据库

```bash
CREATE USER gitea WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE gitea OWNER gitea;
GRANT ALL PRIVILEGES ON DATABASE gitea TO gitea;
```

- helm安装

```bash
helm install --name gitea --namespace env gitea -f myvalues.yaml
```

- 二进制安装

```bash
wget -O gitea https://github.com/go-gitea/gitea/releases/download/v1.5.1/gitea-1.5.1-linux-amd64
chmod +x gitea && mv gitea /usr/local/bin

useradd -m git -p p@ssw0rd

cat > /etc/systemd/system/gitea.service <<EOF
[Unit]
Description=Gitea (Git with a cup of tea)
After=syslog.target
After=network.target
#After=mysqld.service
#After=postgresql.service
#After=memcached.service
#After=redis.service

[Service]
# Modify these two values and uncomment them if you have
# repos with lots of files and get an HTTP error 500 because
# of that
###
#LimitMEMLOCK=infinity
#LimitNOFILE=65535
RestartSec=2s
Type=simple
User=git
Group=git
WorkingDirectory=/var/lib/gitea/
ExecStart=/usr/local/bin/gitea web -c /etc/gitea/app.ini
Restart=always
Environment=USER=git HOME=/home/git GITEA_WORK_DIR=/var/lib/gitea
# If you want to bind Gitea to a port below 1024 uncomment
# the two values below
###
#CapabilityBoundingSet=CAP_NET_BIND_SERVICE
#AmbientCapabilities=CAP_NET_BIND_SERVICE

[Install]
WantedBy=multi-user.target

EOF

systemctl enable gitea
systemctl start gitea
```

app.ini 

```bash
APP_NAME = Gitea: Git with a cup of tea
RUN_USER = git
RUN_MODE = prod

[security]
INTERNAL_TOKEN = eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYmYiOjE1MzY5OTA5ODV9.7pwfC3k5pvJ6Y56-7R6cqFBiqYeCgbWSIRtOZDn7QCU
INSTALL_LOCK   = true
SECRET_KEY     = ln54UFe1A8Hh2l22AE8mG3nla2nkQWVZFIhSt97PNZHLIldC1Kxvy2v438fRue6x

[database]
DB_TYPE  = postgres
HOST     = 172.30.80.90:5432
NAME     = gitea
USER     = gitea
PASSWD   = p@ssw0rd
SSL_MODE = disable

[repository]
ROOT = /opt/gitea/data/gitea-repositories

[server]
SSH_DOMAIN       = gitea.ssii.com
DOMAIN           = gitea.ssii.com
HTTP_ADDR        = 0.0.0.0
HTTP_PORT        = 3000
ROOT_URL         = https://gitea.ssii.com/
DISABLE_SSH      = false
SSH_PORT         = 22
LFS_START_SERVER = true
LFS_CONTENT_PATH = /opt/gitea/data/lfs
LFS_JWT_SECRET   = G6JY7eDefN_g_nh7HBP5Gurfsk89QmwacZOCVtXZFDs
OFFLINE_MODE     = false

[mailer]
ENABLED = false

[service]
REGISTER_EMAIL_CONFIRM            = false
ENABLE_NOTIFY_MAIL                = false
DISABLE_REGISTRATION              = false
ALLOW_ONLY_EXTERNAL_REGISTRATION  = false
ENABLE_CAPTCHA                    = true
REQUIRE_SIGNIN_VIEW               = false
DEFAULT_KEEP_EMAIL_PRIVATE        = false
DEFAULT_ALLOW_CREATE_ORGANIZATION = true
DEFAULT_ENABLE_TIMETRACKING       = true
NO_REPLY_ADDRESS                  = noreply.ssii.com
SSLVERIFY        = false
ENABLE_REVERSE_PROXY_AUTHENTICATION = false
ENABLE_REVERSE_PROXY_AUTO_REGISTRATION = false

[picture]
DISABLE_GRAVATAR        = false
ENABLE_FEDERATED_AVATAR = true

[openid]
ENABLE_OPENID_SIGNIN = true
ENABLE_OPENID_SIGNUP = true

[session]
PROVIDER = file

[log]
MODE      = file
LEVEL     = Info
ROOT_PATH = /var/lib/gitea/log

[other]
SHOW_FOOTER_BRANDING = false
SHOW_FOOTER_VERSION = true
```

配置 nginx 反向代理

```bash

server {
    listen *:80;
    listen *:443 ssl;
    server_name gitea.ssii.com;
    ssl_certificate           /opt/gitea/certs/gitea.cert.pem;
    ssl_certificate_key       /opt/gitea/certs/gitea.key.pem;
    ssl_protocols             TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers               ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA;

    location / {
        proxy_pass http://127.0.0.1:3000;
        #proxy_redirect        off;
        proxy_buffering       off;
        proxy_set_header  Host $host;
        proxy_set_header  X-Real-IP $remote_addr;
        proxy_set_header  X-Forwarded-Proto $scheme;
        proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header  X-Forwarded-Host $server_name;
        fastcgi_param     X-WEBAUTH-USER   $remote_user;
        client_max_body_size 50m;

        # Force https
        #if ($scheme = http) {
        #    rewrite ^ https://$server_name$request_uri? permanent;
        #}
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        #more_clear_input_headers 'Accept-Encoding';
    }
}
```

##### gogs

首先创建数据库

```bash
CREATE USER gogs WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE gogs OWNER gogs;
GRANT ALL PRIVILEGES ON DATABASE gogs TO gogs;
```

然后helm安装

```bash
helm install --name gogs --namespace env mirror/gogs -f myvalues.yaml
```

##### SonarQube

首先创建数据库

```bash
CREATE USER sonarUser WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE sonarDB OWNER sonarUser;
GRANT ALL PRIVILEGES ON DATABASE sonarDB TO sonarUser;
```

然后helm安装

```bash
helm install --name sonarqube --namespace env mirror/sonarqube -f myvalues.yaml
```

##### Drone



#### 3.1.8 PostgreSQL

```bash
helm install --name postgres mirror/postgresql --namespace env --set postgresqlUser=postgres,postgresqlPassword=p@ssw0rd,postgresqlDatabase=postgres,metrics.enabled=true,service.type=LoadBalancer,nodeSelector."node-type"=master,tolerations[0].key=node-role.kubernetes.io/master,tolerations[0].operator=Equal,tolerations[0].value="",tolerations[0].effect=NoSchedule
```

或者

```bash
helm install --name postgres stable/postgresql --namespace env -f myvalues.yaml
```

生成secret

```bash
# postgres_password
echo -n "p@ssw0rd"| base64
# postgres_userpassword
echo -n "postgres:p@ssw0rd"| base64

```

```bash
cat | kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: env
type: Opaque
data:
  postgres_password: cEBzc3cwcmQ=
  postgres_userpassword: cG9zdGdyZXM6cEBzc3cwcmQ=
EOF
```

#### 3.1.9 Keycloak

创建数据库

```bash
CREATE USER keycloak WITH PASSWORD 'p@ssw0rd';
CREATE DATABASE keycloak OWNER keycloak;
GRANT ALL PRIVILEGES ON DATABASE keycloak TO keycloak;
```

生成secret

```bash
# keycloak_password
echo -n "p@ssw0rd"| base64
# keycloak_userpassword
echo -n "keycloak:p@ssw0rd"| base64

```

```bash
cat | kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: keycloak-secret
  namespace: keycloak
type: Opaque
data:
  keycloak_password: cEBzc3cwcmQ=
  keycloak_userpassword: a2V5Y2xvYWs6cEBzc3cwcmQ=
EOF
```

然后helm安装

```bash
helm repo add codecentric https://codecentric.github.io/helm-charts
```

```bash
helm install --name keycloak codecentric/keycloak --namespace keycloak --set keycloak.password="p@ssw0rd",keycloak.ingress.enabled=true,keycloak.ingress.annotations="kubernetes.io/ingress.class: nginx",keycloak.ingress.hosts=[keycloak.ssii.com],keycloak.persistence.deployPostgres=false,keycloak.persistence.dbVendor=postgres,postgresql.postgresUser=keycloak,postgresql.postgresPassword="p@ssw0rd",postgresql.postgresDatabase=keycloak,nodeSelector."node-type"=master,tolerations[0].key=node-role.kubernetes.io/master,tolerations[0].operator=Equal,tolerations[0].value="",tolerations[0].effect=NoSchedule
```

或者

```bash
helm install --name keycloak codecentric/keycloak --namespace keycloak -f myvalues.yaml
```

安装后配置 ldap 为 user provider

#### 3.1.10 mysql

```bash
helm install --name mysql --namespace env -f config/mysql/myvalues.yaml stable/mysql
```

#### 3.1.11 mariadb

```bash
helm install --name mariadb --namespace env -f config/mariadb/myvalues.yaml stable/mariadb
```



### 3.2 istio

~~helm安装，并默认开启tls~~

```bash
kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml
kubectl apply -f install/kubernetes/helm/istio/charts/certmanager/templates/crds.yaml

helm install install/kubernetes/helm/istio --name istio --namespace istio-system #--set global.mtls.enabled=true
```

更新：manifest 安装

```bash
$ helm template install/kubernetes/helm/istio \
--name istio --namespace istio-system \
--set galley.enabled=true \
--set global.k8sIngress.enabled=true \
--set grafana.enabled=true \
--set grafana.security.enabled=true \
--set grafana.persist=true \
--set kiali.enabled=true \
--set kiali.ingress.enabled=true \
--set kiali.createDemoSecret=true \
--set prometheus.ingress.enabled=true \
--set tracing.enabled=true \
--set tracing.ingress.enabled=true > istio.yaml
# edit properties in istio.yaml
```

增加 ingress 配置，然后执行安装

```bash
$ kubectl create namespace istio-system
$ kubectl create -f istio.yaml
$ kubectl create -f metrics-ingress.yaml
```

管理员账号密码

```bash
# kiali
username: admin	passphrase: admin123
# grafana
username: admin	passphrase: admin
```

```bash
#kubectl get configmap istio -o yaml -n istio-system | grep authPolicy | head -1
kubectl get policies.authentication.istio.io -n default -o yaml
kubectl get destinationrules.networking.istio.io --all-namespaces -o yaml
```

- 升级 istio

```bash
kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system
# or same as manifest installation
helm template install/kubernetes/helm/istio --name istio \
    --namespace istio-system > istio.yaml
kubectl apply -f istio.yaml
```

- 安装 sample

```bash
kubectl label namespace default istio-injection=enabled
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
# kubectl get svc,pod -o wide

kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
# kubectl get gateway
# kubectl get svc istio-ingressgateway -n istio-system
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
# export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT

# curl -o /dev/null -s -w "%{http_code}\n" http://${GATEWAY_URL}/productpage

kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml
# kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
# kubectl get destinationrules -o yaml
```

### 3.3 conduit

预先下载 conduit 的 docker image

安装 conduit cli

```bash
curl https://run.conduit.io/install | sh
export PATH=$PATH:$HOME/.conduit/bin

```

安装 conduit 到 k8s 集群

```bash
conduit install | kubectl apply -f -
conduit version
conduit dashboard
```

安装 demo app

```bash
curl https://raw.githubusercontent.com/runconduit/conduit-examples/master/emojivoto/emojivoto.yml | conduit inject - | kubectl apply -f -
```



### 3.4 spinnaker

#### 3.4.1 安装过程



#### 3.4.2 demo



### 3.5 开发PaaS安装配置(optional)

#### 3.5.1 jboss fabric8 (optional)

- 参数和命令：
  - gofabric8 启动时环境配置：gofabric8 start --memory=2000 --cpus=2 --disk-size=30g
  - 关闭Hyper-V错误提示：minikube config set WantReportErrorPrompt false 
- 安装配置流程：

```mermaid
graph TB
	st((开始)) --> io1>硬件和操作系统<br/>兼容性清单]
	io1 --> op1(检查硬件虚拟化是否开启<br/>和操作系统兼容性)
	op1 --> cond1{是/否}
	cond1 --否--> io1
	cond1 --是--> io2>软件兼容性清单]
	io2 --> op2(检查是否已安装<br/>和配置Docker)
	op2 --> cond2{是/否}
	cond2 --是--> op3(检查是否已安装<br/>和配置Kubernates)
	cond2 --否--> op4(安装和配置Docker)
	op4 --> op3
	op3 --> cond3{是/否}
	cond3 --是--> op5(检查是否已安装<br/>和配置fabric8)
	cond3 --否--> op6(安装和配置Kubernates)
	op6 --> op5
	op5 --> cond4{是/否}
	cond4 --是--> op7(配置fabric8参数<br/>和自定义属性)
	cond4 --否--> op8(安装fabric8)
	op8 --> op7
	op7 --> io3>fabric8配置和命令参数清单]
	io3 --> op9(配置fabric8 dashboard<br/>和各种配套工具)
	op9 --> e((结束))
```

#### 3.5.2 flynn (optional)

## 4. DevOps

### 4.1 pipeline



